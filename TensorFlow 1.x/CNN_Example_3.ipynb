{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1QjMlVl4Y8e"
   },
   "source": [
    "CNN_Example_3\n",
    "\n",
    "1 conv / 1 flatten CNN architecture\n",
    "\n",
    "5 x 5 x 32 filter, padding 있음, 2 x 2 max pooling\n",
    "\n",
    "index_label_prediction list 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "d4_8QFFN4Yq5",
    "outputId": "2ceec2da-48d2-4015-82fb-4f7f8c06c5e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "8vsMvkVe45g5",
    "outputId": "03e43f24-8419-4add-ba62-5ba1d1624549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-026760e93c65>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\n",
      "train.num =  55000 , test.num =  10000 , validation.num =  5000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"train.num = \", mnist.train.num_examples, \n",
    "      \", test.num = \", mnist.test.num_examples, \n",
    "      \", validation.num = \", mnist.validation.num_examples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "VO-KKjMj5LQ1",
    "outputId": "4b55ce4b-3455-4f14-de1b-721e7b611f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(mnist) =  <class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'> , type(mnist.train.images) =  <class 'numpy.ndarray'> , type(mnist.train.labels) =  <class 'numpy.ndarray'>\n",
      "\n",
      "train image shape =  (55000, 784)\n",
      "train label shape =  (55000, 10)\n",
      "test image shape =  (10000, 784)\n",
      "test label shape =  (10000, 10)\n",
      "\n",
      "train image shape =  (55000, 784)\n",
      "test image shape =  (10000, 784)\n",
      "validation image shape =  (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "# shape, type 확인\n",
    "\n",
    "print(\"type(mnist) = \", type(mnist), \n",
    "      \", type(mnist.train.images) = \", type(mnist.train.images), \n",
    "      \", type(mnist.train.labels) = \", type(mnist.train.labels))\n",
    "\n",
    "print(\"\\ntrain image shape = \", np.shape(mnist.train.images))\n",
    "print(\"train label shape = \", np.shape(mnist.train.labels))\n",
    "print(\"test image shape = \", np.shape(mnist.test.images))\n",
    "print(\"test label shape = \", np.shape(mnist.test.labels))\n",
    "\n",
    "print(\"\\ntrain image shape = \", mnist.train.images.shape)\n",
    "print(\"test image shape = \", mnist.test.images.shape)\n",
    "print(\"validation image shape = \", mnist.validation.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mZ09WZEl5WaN",
    "outputId": "1a73494b-ae1d-423b-afde-b9138d15c381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "정규화 확인\n",
      "\n",
      "len(mnist.train.images[0]) =  784\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "=====================================================================\n",
      "One-Hot Encoding 확인\n",
      "\n",
      "len(mnist.train.labels[0]) =  10\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# train data 정규화 및 label 의 one-hot encoding 확인\n",
    "\n",
    "print('=====================================================================')\n",
    "print('정규화 확인\\n')\n",
    "print('len(mnist.train.images[0]) = ', len(mnist.train.images[0]))\n",
    "print(mnist.train.images[0])\n",
    "print('=====================================================================')\n",
    "print('One-Hot Encoding 확인\\n')\n",
    "print('len(mnist.train.labels[0]) = ', len(mnist.train.labels[0]))\n",
    "print(mnist.train.labels[0])  # One-Hot Encoding 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTTWxdT85gjL"
   },
   "outputs": [],
   "source": [
    "# Hyper-Parameter\n",
    "learning_rate = 0.001  # 학습율\n",
    "epochs = 50            # 반복횟수\n",
    "batch_size = 100      # 한번에 입력으로 주어지는 MNIST 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwaglZAw54ER"
   },
   "outputs": [],
   "source": [
    "# 입력과 출력을 위한 플레이스홀더 정의\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])  \n",
    "T = tf.placeholder(tf.float32, [None, 10])  \n",
    "\n",
    "# 입력층의 출력 값, 컨볼루션 연산을 위해 reshape 시킴\n",
    "A1 = X_img = tf.reshape(X, [-1, 28, 28, 1])   # image 28 x 28 x 1 (black/white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eShGy8Nb6ctx"
   },
   "outputs": [],
   "source": [
    "# 1번째 컨볼루션 층, 5X5X32 필터\n",
    "W2 = tf.Variable(tf.random_normal([5, 5, 1, 32], stddev=0.01))  \n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[32]))  \n",
    "\n",
    "# 1번째 컨볼루션 연산을 통해 28 X 28 X1  => 28 X 28 X 32 \n",
    "C2 = tf.nn.conv2d(A1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# relu\n",
    "Z2 = tf.nn.relu(C2+b2)\n",
    "\n",
    "# 1번째 max pooling을 통해 28 X 28 X 32  => 14 X 14 X 32 \n",
    "A2 = P2 = tf.nn.max_pool(Z2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8si7scby61Ww"
   },
   "outputs": [],
   "source": [
    "# 14X14 크기를 가진 32개의 activation map을 flatten 시킴\n",
    "A2_flat = P2_flat = tf.reshape(A2, [-1, 14*14*32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2r8jSFT67UIC"
   },
   "outputs": [],
   "source": [
    "# 출력층\n",
    "W3 = tf.Variable(tf.random_normal([14*14*32, 10], stddev=0.01))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# 출력층 선형회귀  값 Z3, 즉 softmax 에 들어가는 입력 값\n",
    "Z3 = logits = tf.matmul(A2_flat, W3) + b3    # 선형회귀 값 Z3\n",
    "\n",
    "y = A3 = tf.nn.softmax(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-i1Pgp97d6K"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=T) )\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "train = optimizer.minimize(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4lMip_D7iI1"
   },
   "outputs": [],
   "source": [
    "# batch_size X 10 데이터에 대해 argmax를 통해 행단위로 비교함\n",
    "predicted_val = tf.equal( tf.argmax(A3, 1), tf.argmax(T, 1) )\n",
    "\n",
    "# batch_size X 10 의 True, False 를 1 또는 0 으로 변환\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))\n",
    "\n",
    "# index list 출력\n",
    "accuracy_index = tf.cast(predicted_val, dtype=tf.float32)\n",
    "\n",
    "# 예측값 처리\n",
    "predicted_list = tf.argmax(A3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HHeMifOe7nKF",
    "outputId": "5403a1dd-8891-4cbd-fd36-6abad43426b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  0 , step =  0 , loss_val =  2.7070303\n",
      "epochs =  0 , step =  100 , loss_val =  0.41290936\n",
      "epochs =  0 , step =  200 , loss_val =  0.26557866\n",
      "epochs =  0 , step =  300 , loss_val =  0.09306531\n",
      "epochs =  0 , step =  400 , loss_val =  0.16708393\n",
      "epochs =  0 , step =  500 , loss_val =  0.15130913\n",
      "epochs =  1 , step =  0 , loss_val =  0.10698189\n",
      "epochs =  1 , step =  100 , loss_val =  0.15415779\n",
      "epochs =  1 , step =  200 , loss_val =  0.10778224\n",
      "epochs =  1 , step =  300 , loss_val =  0.07953584\n",
      "epochs =  1 , step =  400 , loss_val =  0.045113783\n",
      "epochs =  1 , step =  500 , loss_val =  0.061269347\n",
      "epochs =  2 , step =  0 , loss_val =  0.07880659\n",
      "epochs =  2 , step =  100 , loss_val =  0.19698367\n",
      "epochs =  2 , step =  200 , loss_val =  0.06816709\n",
      "epochs =  2 , step =  300 , loss_val =  0.108196974\n",
      "epochs =  2 , step =  400 , loss_val =  0.11632169\n",
      "epochs =  2 , step =  500 , loss_val =  0.035891656\n",
      "epochs =  3 , step =  0 , loss_val =  0.07757712\n",
      "epochs =  3 , step =  100 , loss_val =  0.1270608\n",
      "epochs =  3 , step =  200 , loss_val =  0.052111708\n",
      "epochs =  3 , step =  300 , loss_val =  0.09028405\n",
      "epochs =  3 , step =  400 , loss_val =  0.053091258\n",
      "epochs =  3 , step =  500 , loss_val =  0.071723714\n",
      "epochs =  4 , step =  0 , loss_val =  0.030502718\n",
      "epochs =  4 , step =  100 , loss_val =  0.06684062\n",
      "epochs =  4 , step =  200 , loss_val =  0.11239104\n",
      "epochs =  4 , step =  300 , loss_val =  0.014922189\n",
      "epochs =  4 , step =  400 , loss_val =  0.05720932\n",
      "epochs =  4 , step =  500 , loss_val =  0.07614939\n",
      "epochs =  5 , step =  0 , loss_val =  0.0051334035\n",
      "epochs =  5 , step =  100 , loss_val =  0.020254308\n",
      "epochs =  5 , step =  200 , loss_val =  0.008112449\n",
      "epochs =  5 , step =  300 , loss_val =  0.066394724\n",
      "epochs =  5 , step =  400 , loss_val =  0.028026791\n",
      "epochs =  5 , step =  500 , loss_val =  0.044237603\n",
      "epochs =  6 , step =  0 , loss_val =  0.07025194\n",
      "epochs =  6 , step =  100 , loss_val =  0.023710603\n",
      "epochs =  6 , step =  200 , loss_val =  0.014447452\n",
      "epochs =  6 , step =  300 , loss_val =  0.005056119\n",
      "epochs =  6 , step =  400 , loss_val =  0.079443365\n",
      "epochs =  6 , step =  500 , loss_val =  0.102747075\n",
      "epochs =  7 , step =  0 , loss_val =  0.061941408\n",
      "epochs =  7 , step =  100 , loss_val =  0.04127009\n",
      "epochs =  7 , step =  200 , loss_val =  0.0029170227\n",
      "epochs =  7 , step =  300 , loss_val =  0.1077311\n",
      "epochs =  7 , step =  400 , loss_val =  0.017409377\n",
      "epochs =  7 , step =  500 , loss_val =  0.019503647\n",
      "epochs =  8 , step =  0 , loss_val =  0.00909625\n",
      "epochs =  8 , step =  100 , loss_val =  0.02464886\n",
      "epochs =  8 , step =  200 , loss_val =  0.041365366\n",
      "epochs =  8 , step =  300 , loss_val =  0.03654426\n",
      "epochs =  8 , step =  400 , loss_val =  0.043755885\n",
      "epochs =  8 , step =  500 , loss_val =  0.02258178\n",
      "epochs =  9 , step =  0 , loss_val =  0.006009321\n",
      "epochs =  9 , step =  100 , loss_val =  0.008903105\n",
      "epochs =  9 , step =  200 , loss_val =  0.0013859511\n",
      "epochs =  9 , step =  300 , loss_val =  0.017697038\n",
      "epochs =  9 , step =  400 , loss_val =  0.057840496\n",
      "epochs =  9 , step =  500 , loss_val =  0.014967152\n",
      "epochs =  10 , step =  0 , loss_val =  0.0026968366\n",
      "epochs =  10 , step =  100 , loss_val =  0.012121353\n",
      "epochs =  10 , step =  200 , loss_val =  0.084933415\n",
      "epochs =  10 , step =  300 , loss_val =  0.04172502\n",
      "epochs =  10 , step =  400 , loss_val =  0.00590855\n",
      "epochs =  10 , step =  500 , loss_val =  0.0067492058\n",
      "epochs =  11 , step =  0 , loss_val =  0.005077524\n",
      "epochs =  11 , step =  100 , loss_val =  0.014713455\n",
      "epochs =  11 , step =  200 , loss_val =  0.008752752\n",
      "epochs =  11 , step =  300 , loss_val =  0.017708397\n",
      "epochs =  11 , step =  400 , loss_val =  0.05659294\n",
      "epochs =  11 , step =  500 , loss_val =  0.005681274\n",
      "epochs =  12 , step =  0 , loss_val =  0.010501262\n",
      "epochs =  12 , step =  100 , loss_val =  0.012888366\n",
      "epochs =  12 , step =  200 , loss_val =  0.035835892\n",
      "epochs =  12 , step =  300 , loss_val =  0.008641219\n",
      "epochs =  12 , step =  400 , loss_val =  0.008792438\n",
      "epochs =  12 , step =  500 , loss_val =  0.020139506\n",
      "epochs =  13 , step =  0 , loss_val =  0.002300417\n",
      "epochs =  13 , step =  100 , loss_val =  0.006466676\n",
      "epochs =  13 , step =  200 , loss_val =  0.0031656\n",
      "epochs =  13 , step =  300 , loss_val =  0.012457031\n",
      "epochs =  13 , step =  400 , loss_val =  0.007822027\n",
      "epochs =  13 , step =  500 , loss_val =  0.016483372\n",
      "epochs =  14 , step =  0 , loss_val =  0.00527778\n",
      "epochs =  14 , step =  100 , loss_val =  0.0053781862\n",
      "epochs =  14 , step =  200 , loss_val =  0.0062665776\n",
      "epochs =  14 , step =  300 , loss_val =  0.0051190346\n",
      "epochs =  14 , step =  400 , loss_val =  0.03335455\n",
      "epochs =  14 , step =  500 , loss_val =  0.0027228426\n",
      "epochs =  15 , step =  0 , loss_val =  0.04613613\n",
      "epochs =  15 , step =  100 , loss_val =  0.0072225607\n",
      "epochs =  15 , step =  200 , loss_val =  0.036329463\n",
      "epochs =  15 , step =  300 , loss_val =  0.008317936\n",
      "epochs =  15 , step =  400 , loss_val =  0.0042432044\n",
      "epochs =  15 , step =  500 , loss_val =  0.01578765\n",
      "epochs =  16 , step =  0 , loss_val =  0.004542286\n",
      "epochs =  16 , step =  100 , loss_val =  0.0015749664\n",
      "epochs =  16 , step =  200 , loss_val =  0.0016980731\n",
      "epochs =  16 , step =  300 , loss_val =  0.006750406\n",
      "epochs =  16 , step =  400 , loss_val =  0.01666859\n",
      "epochs =  16 , step =  500 , loss_val =  0.0038848529\n",
      "epochs =  17 , step =  0 , loss_val =  0.017333439\n",
      "epochs =  17 , step =  100 , loss_val =  0.0021459565\n",
      "epochs =  17 , step =  200 , loss_val =  0.0005652455\n",
      "epochs =  17 , step =  300 , loss_val =  0.009548281\n",
      "epochs =  17 , step =  400 , loss_val =  0.0032760454\n",
      "epochs =  17 , step =  500 , loss_val =  0.005799063\n",
      "epochs =  18 , step =  0 , loss_val =  0.0013942122\n",
      "epochs =  18 , step =  100 , loss_val =  0.0043472154\n",
      "epochs =  18 , step =  200 , loss_val =  0.0023937447\n",
      "epochs =  18 , step =  300 , loss_val =  0.019949622\n",
      "epochs =  18 , step =  400 , loss_val =  0.003391594\n",
      "epochs =  18 , step =  500 , loss_val =  0.0037429426\n",
      "epochs =  19 , step =  0 , loss_val =  0.012626243\n",
      "epochs =  19 , step =  100 , loss_val =  0.0044277376\n",
      "epochs =  19 , step =  200 , loss_val =  0.010022543\n",
      "epochs =  19 , step =  300 , loss_val =  0.0025558225\n",
      "epochs =  19 , step =  400 , loss_val =  0.016502127\n",
      "epochs =  19 , step =  500 , loss_val =  0.01763372\n",
      "epochs =  20 , step =  0 , loss_val =  0.0075160647\n",
      "epochs =  20 , step =  100 , loss_val =  0.05340701\n",
      "epochs =  20 , step =  200 , loss_val =  0.004488564\n",
      "epochs =  20 , step =  300 , loss_val =  0.00851066\n",
      "epochs =  20 , step =  400 , loss_val =  0.0026337078\n",
      "epochs =  20 , step =  500 , loss_val =  0.004772851\n",
      "epochs =  21 , step =  0 , loss_val =  0.0026000524\n",
      "epochs =  21 , step =  100 , loss_val =  0.0077098506\n",
      "epochs =  21 , step =  200 , loss_val =  0.015839154\n",
      "epochs =  21 , step =  300 , loss_val =  0.0046213185\n",
      "epochs =  21 , step =  400 , loss_val =  0.009990112\n",
      "epochs =  21 , step =  500 , loss_val =  0.0013723127\n",
      "epochs =  22 , step =  0 , loss_val =  0.0014973439\n",
      "epochs =  22 , step =  100 , loss_val =  0.00087927224\n",
      "epochs =  22 , step =  200 , loss_val =  0.0040230965\n",
      "epochs =  22 , step =  300 , loss_val =  0.00028645582\n",
      "epochs =  22 , step =  400 , loss_val =  0.0155750085\n",
      "epochs =  22 , step =  500 , loss_val =  0.00018347202\n",
      "epochs =  23 , step =  0 , loss_val =  0.0008722724\n",
      "epochs =  23 , step =  100 , loss_val =  0.0009719932\n",
      "epochs =  23 , step =  200 , loss_val =  0.0012450224\n",
      "epochs =  23 , step =  300 , loss_val =  0.0071577774\n",
      "epochs =  23 , step =  400 , loss_val =  0.0010048274\n",
      "epochs =  23 , step =  500 , loss_val =  0.0050717564\n",
      "epochs =  24 , step =  0 , loss_val =  0.0012452586\n",
      "epochs =  24 , step =  100 , loss_val =  0.0012838416\n",
      "epochs =  24 , step =  200 , loss_val =  0.0013477436\n",
      "epochs =  24 , step =  300 , loss_val =  0.000857045\n",
      "epochs =  24 , step =  400 , loss_val =  0.0053809066\n",
      "epochs =  24 , step =  500 , loss_val =  0.013635424\n",
      "epochs =  25 , step =  0 , loss_val =  0.00040262032\n",
      "epochs =  25 , step =  100 , loss_val =  0.0005117018\n",
      "epochs =  25 , step =  200 , loss_val =  0.0024594434\n",
      "epochs =  25 , step =  300 , loss_val =  0.0007026458\n",
      "epochs =  25 , step =  400 , loss_val =  0.0015997294\n",
      "epochs =  25 , step =  500 , loss_val =  0.0006353957\n",
      "epochs =  26 , step =  0 , loss_val =  0.00094809826\n",
      "epochs =  26 , step =  100 , loss_val =  0.00062577357\n",
      "epochs =  26 , step =  200 , loss_val =  0.0012169391\n",
      "epochs =  26 , step =  300 , loss_val =  0.00015412849\n",
      "epochs =  26 , step =  400 , loss_val =  0.0012068298\n",
      "epochs =  26 , step =  500 , loss_val =  0.0026566407\n",
      "epochs =  27 , step =  0 , loss_val =  0.0050306064\n",
      "epochs =  27 , step =  100 , loss_val =  0.0014011806\n",
      "epochs =  27 , step =  200 , loss_val =  0.0006747825\n",
      "epochs =  27 , step =  300 , loss_val =  0.00045944192\n",
      "epochs =  27 , step =  400 , loss_val =  0.0013035983\n",
      "epochs =  27 , step =  500 , loss_val =  9.253724e-05\n",
      "epochs =  28 , step =  0 , loss_val =  0.0002175115\n",
      "epochs =  28 , step =  100 , loss_val =  0.005563022\n",
      "epochs =  28 , step =  200 , loss_val =  0.0027257695\n",
      "epochs =  28 , step =  300 , loss_val =  0.0038215192\n",
      "epochs =  28 , step =  400 , loss_val =  0.00024091854\n",
      "epochs =  28 , step =  500 , loss_val =  0.00022331825\n",
      "epochs =  29 , step =  0 , loss_val =  0.00052981474\n",
      "epochs =  29 , step =  100 , loss_val =  0.0045986446\n",
      "epochs =  29 , step =  200 , loss_val =  0.0013728886\n",
      "epochs =  29 , step =  300 , loss_val =  0.0009130058\n",
      "epochs =  29 , step =  400 , loss_val =  0.014072449\n",
      "epochs =  29 , step =  500 , loss_val =  0.011756868\n",
      "epochs =  30 , step =  0 , loss_val =  0.012675978\n",
      "epochs =  30 , step =  100 , loss_val =  2.3187742e-05\n",
      "epochs =  30 , step =  200 , loss_val =  0.0038155743\n",
      "epochs =  30 , step =  300 , loss_val =  0.0011813175\n",
      "epochs =  30 , step =  400 , loss_val =  0.0004172652\n",
      "epochs =  30 , step =  500 , loss_val =  0.0001469872\n",
      "epochs =  31 , step =  0 , loss_val =  0.0008747431\n",
      "epochs =  31 , step =  100 , loss_val =  0.00051198195\n",
      "epochs =  31 , step =  200 , loss_val =  0.00057115\n",
      "epochs =  31 , step =  300 , loss_val =  0.001131975\n",
      "epochs =  31 , step =  400 , loss_val =  3.032409e-05\n",
      "epochs =  31 , step =  500 , loss_val =  0.0007291534\n",
      "epochs =  32 , step =  0 , loss_val =  0.0004581743\n",
      "epochs =  32 , step =  100 , loss_val =  0.0005295204\n",
      "epochs =  32 , step =  200 , loss_val =  0.0004312637\n",
      "epochs =  32 , step =  300 , loss_val =  0.00022012985\n",
      "epochs =  32 , step =  400 , loss_val =  0.0009714403\n",
      "epochs =  32 , step =  500 , loss_val =  0.00080170314\n",
      "epochs =  33 , step =  0 , loss_val =  0.0004186109\n",
      "epochs =  33 , step =  100 , loss_val =  0.00022111196\n",
      "epochs =  33 , step =  200 , loss_val =  0.0014160918\n",
      "epochs =  33 , step =  300 , loss_val =  0.0003216005\n",
      "epochs =  33 , step =  400 , loss_val =  0.0026991246\n",
      "epochs =  33 , step =  500 , loss_val =  0.000564708\n",
      "epochs =  34 , step =  0 , loss_val =  0.0005874322\n",
      "epochs =  34 , step =  100 , loss_val =  0.00018984044\n",
      "epochs =  34 , step =  200 , loss_val =  0.00039768813\n",
      "epochs =  34 , step =  300 , loss_val =  0.00042112998\n",
      "epochs =  34 , step =  400 , loss_val =  7.785207e-05\n",
      "epochs =  34 , step =  500 , loss_val =  0.00019734673\n",
      "epochs =  35 , step =  0 , loss_val =  0.00026648745\n",
      "epochs =  35 , step =  100 , loss_val =  0.0011452504\n",
      "epochs =  35 , step =  200 , loss_val =  0.00036735125\n",
      "epochs =  35 , step =  300 , loss_val =  0.0006075764\n",
      "epochs =  35 , step =  400 , loss_val =  0.00030940733\n",
      "epochs =  35 , step =  500 , loss_val =  0.014582496\n",
      "epochs =  36 , step =  0 , loss_val =  0.003977255\n",
      "epochs =  36 , step =  100 , loss_val =  0.00112917\n",
      "epochs =  36 , step =  200 , loss_val =  0.00023541559\n",
      "epochs =  36 , step =  300 , loss_val =  0.00018628869\n",
      "epochs =  36 , step =  400 , loss_val =  0.0014564083\n",
      "epochs =  36 , step =  500 , loss_val =  0.0011866097\n",
      "epochs =  37 , step =  0 , loss_val =  0.0026588123\n",
      "epochs =  37 , step =  100 , loss_val =  3.1851607e-05\n",
      "epochs =  37 , step =  200 , loss_val =  0.001173384\n",
      "epochs =  37 , step =  300 , loss_val =  0.0006087905\n",
      "epochs =  37 , step =  400 , loss_val =  0.0011727014\n",
      "epochs =  37 , step =  500 , loss_val =  0.00018794095\n",
      "epochs =  38 , step =  0 , loss_val =  0.00093777495\n",
      "epochs =  38 , step =  100 , loss_val =  0.0004710849\n",
      "epochs =  38 , step =  200 , loss_val =  0.00027041417\n",
      "epochs =  38 , step =  300 , loss_val =  2.9538909e-05\n",
      "epochs =  38 , step =  400 , loss_val =  6.190901e-05\n",
      "epochs =  38 , step =  500 , loss_val =  0.00015887943\n",
      "epochs =  39 , step =  0 , loss_val =  0.00015206992\n",
      "epochs =  39 , step =  100 , loss_val =  0.0004241031\n",
      "epochs =  39 , step =  200 , loss_val =  0.0002247247\n",
      "epochs =  39 , step =  300 , loss_val =  0.00013545295\n",
      "epochs =  39 , step =  400 , loss_val =  5.2379077e-05\n",
      "epochs =  39 , step =  500 , loss_val =  9.961973e-05\n",
      "epochs =  40 , step =  0 , loss_val =  0.0002770342\n",
      "epochs =  40 , step =  100 , loss_val =  0.00012513153\n",
      "epochs =  40 , step =  200 , loss_val =  5.240239e-05\n",
      "epochs =  40 , step =  300 , loss_val =  2.0956356e-06\n",
      "epochs =  40 , step =  400 , loss_val =  0.00013685683\n",
      "epochs =  40 , step =  500 , loss_val =  0.0008410537\n",
      "epochs =  41 , step =  0 , loss_val =  5.150873e-05\n",
      "epochs =  41 , step =  100 , loss_val =  0.0012452282\n",
      "epochs =  41 , step =  200 , loss_val =  0.0011141368\n",
      "epochs =  41 , step =  300 , loss_val =  9.832569e-05\n",
      "epochs =  41 , step =  400 , loss_val =  0.03700606\n",
      "epochs =  41 , step =  500 , loss_val =  0.0029056075\n",
      "epochs =  42 , step =  0 , loss_val =  0.0017989196\n",
      "epochs =  42 , step =  100 , loss_val =  0.0010862957\n",
      "epochs =  42 , step =  200 , loss_val =  1.0241771e-05\n",
      "epochs =  42 , step =  300 , loss_val =  0.0001096796\n",
      "epochs =  42 , step =  400 , loss_val =  0.0020811448\n",
      "epochs =  42 , step =  500 , loss_val =  2.6368503e-05\n",
      "epochs =  43 , step =  0 , loss_val =  0.00021026698\n",
      "epochs =  43 , step =  100 , loss_val =  0.00010577316\n",
      "epochs =  43 , step =  200 , loss_val =  0.00065893045\n",
      "epochs =  43 , step =  300 , loss_val =  6.387919e-05\n",
      "epochs =  43 , step =  400 , loss_val =  2.0678644e-05\n",
      "epochs =  43 , step =  500 , loss_val =  0.00016751485\n",
      "epochs =  44 , step =  0 , loss_val =  0.00022872961\n",
      "epochs =  44 , step =  100 , loss_val =  8.887436e-05\n",
      "epochs =  44 , step =  200 , loss_val =  3.6392958e-06\n",
      "epochs =  44 , step =  300 , loss_val =  7.356216e-05\n",
      "epochs =  44 , step =  400 , loss_val =  1.1900979e-05\n",
      "epochs =  44 , step =  500 , loss_val =  6.411725e-06\n",
      "epochs =  45 , step =  0 , loss_val =  5.7586705e-05\n",
      "epochs =  45 , step =  100 , loss_val =  5.7840753e-05\n",
      "epochs =  45 , step =  200 , loss_val =  0.00032917885\n",
      "epochs =  45 , step =  300 , loss_val =  0.0001992476\n",
      "epochs =  45 , step =  400 , loss_val =  5.1474362e-05\n",
      "epochs =  45 , step =  500 , loss_val =  0.00026634586\n",
      "epochs =  46 , step =  0 , loss_val =  3.529088e-05\n",
      "epochs =  46 , step =  100 , loss_val =  1.6414364e-05\n",
      "epochs =  46 , step =  200 , loss_val =  0.00017409673\n",
      "epochs =  46 , step =  300 , loss_val =  0.00032910873\n",
      "epochs =  46 , step =  400 , loss_val =  2.1282694e-05\n",
      "epochs =  46 , step =  500 , loss_val =  3.940617e-05\n",
      "epochs =  47 , step =  0 , loss_val =  2.6148087e-05\n",
      "epochs =  47 , step =  100 , loss_val =  0.0002472885\n",
      "epochs =  47 , step =  200 , loss_val =  2.9670605e-06\n",
      "epochs =  47 , step =  300 , loss_val =  0.00030934694\n",
      "epochs =  47 , step =  400 , loss_val =  0.0023872713\n",
      "epochs =  47 , step =  500 , loss_val =  0.03790093\n",
      "epochs =  48 , step =  0 , loss_val =  0.0046590418\n",
      "epochs =  48 , step =  100 , loss_val =  0.00042243104\n",
      "epochs =  48 , step =  200 , loss_val =  0.001989557\n",
      "epochs =  48 , step =  300 , loss_val =  9.738357e-06\n",
      "epochs =  48 , step =  400 , loss_val =  7.516672e-05\n",
      "epochs =  48 , step =  500 , loss_val =  8.964807e-05\n",
      "epochs =  49 , step =  0 , loss_val =  0.00019384507\n",
      "epochs =  49 , step =  100 , loss_val =  3.6651098e-05\n",
      "epochs =  49 , step =  200 , loss_val =  0.00021866379\n",
      "epochs =  49 , step =  300 , loss_val =  8.011434e-05\n",
      "epochs =  49 , step =  400 , loss_val =  1.7602388e-05\n",
      "epochs =  49 , step =  500 , loss_val =  1.7007842e-05\n",
      "\n",
      "elapsed time =  0:01:05.666107\n",
      "\n",
      "Accuracy =  0.988\n",
      "type(accuracy_val) =  <class 'numpy.float32'> , type(predicted_list_val) =  <class 'numpy.ndarray'> , type(index_label) =  <class 'numpy.ndarray'>\n",
      "index_label.shape =  (10000,)\n",
      "length of index_label_list =  10000\n",
      "false label count =  120\n",
      "\n",
      "length of index_label_false_list 120\n"
     ]
    }
   ],
   "source": [
    "index_label_prediction_list = []\n",
    "\n",
    "with  tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())  # 변수 노드(tf.Variable) 초기화\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(epochs):    \n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples / batch_size)  # 55,000 / 100\n",
    "\n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "      \n",
    "            loss_val, _ = sess.run([loss, train], feed_dict={X: batch_x_data, T: batch_t_data})    \n",
    "        \n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)             \n",
    "    \n",
    "    end_time = datetime.now() \n",
    "    \n",
    "    print(\"\\nelapsed time = \", end_time - start_time) \n",
    "    \n",
    "    # Accuracy 확인\n",
    "    test_x_data = mnist.test.images    # 10000 X 784\n",
    "    test_t_data = mnist.test.labels    # 10000 X 10\n",
    "    \n",
    "    accuracy_val, predicted_list_val, index_label = sess.run([accuracy, predicted_list, accuracy_index], feed_dict={X: test_x_data, T: test_t_data})\n",
    "    \n",
    "    print(\"\\nAccuracy = \", accuracy_val)\n",
    "    print(\"type(accuracy_val) = \", type(accuracy_val), ', type(predicted_list_val) = ', type(predicted_list_val), ', type(index_label) = ', type(index_label))\n",
    "    print(\"index_label.shape = \", index_label.shape)\n",
    "    \n",
    "    index_label_list = list(index_label)\n",
    "    print(\"length of index_label_list = \", len(index_label_list))\n",
    "    print(\"false label count = \", index_label_list.count([0]))\n",
    "        \n",
    "    temp_list = [] \n",
    "    \n",
    "    for index in range(len(index_label)):\n",
    "        \n",
    "        if index_label[index] == 0:\n",
    "            \n",
    "            temp_list.append(index)\n",
    "            temp_list.append(np.argmax(test_t_data[index]))  # one-hot encoding 이므로 argmax 로 정답 추출\n",
    "            temp_list.append(predicted_list_val[index])\n",
    "            \n",
    "            index_label_prediction_list.append(temp_list)\n",
    "            \n",
    "            temp_list = []\n",
    "            \n",
    "    print(\"\\nlength of index_label_false_list\", len(index_label_prediction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "74xULKC78SCu",
    "outputId": "774ce8bd-be5a-455a-a48f-8aa8154f5ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 3, 5], [62, 9, 5], [247, 4, 6], [259, 6, 0], [321, 2, 7], [340, 5, 3], [445, 6, 0], [449, 3, 5], [659, 2, 1], [684, 7, 3], [844, 8, 7], [882, 9, 7], [895, 0, 8], [958, 3, 0], [965, 6, 0], [1014, 6, 5], [1039, 7, 2], [1112, 4, 6], [1202, 8, 6], [1224, 2, 4], [1226, 7, 2], [1232, 9, 4], [1242, 4, 9], [1247, 9, 5], [1260, 7, 1], [1319, 8, 0], [1337, 2, 6], [1393, 5, 3], [1530, 8, 7], [1549, 4, 6], [1621, 0, 6], [1641, 5, 9], [1678, 2, 1], [1709, 9, 3], [1717, 8, 0], [1754, 7, 2], [1790, 2, 7], [1883, 7, 9], [1901, 9, 4], [2004, 8, 9], [2035, 5, 3], [2098, 2, 0], [2118, 6, 0], [2130, 4, 9], [2135, 6, 1], [2293, 9, 4], [2329, 0, 2], [2369, 5, 3], [2387, 9, 1], [2406, 9, 4], [2462, 2, 0], [2597, 5, 3], [2654, 6, 1], [2720, 9, 4], [2896, 8, 0], [2921, 3, 2], [2939, 9, 5], [2953, 3, 5], [2995, 6, 8], [3060, 9, 2], [3330, 2, 8], [3422, 6, 0], [3503, 9, 1], [3520, 6, 4], [3559, 8, 0], [3727, 8, 9], [3751, 7, 2], [3762, 6, 5], [3808, 7, 8], [3850, 9, 4], [3853, 6, 8], [4007, 7, 4], [4176, 2, 7], [4201, 1, 7], [4224, 9, 7], [4238, 7, 3], [4248, 2, 8], [4256, 3, 2], [4289, 2, 7], [4405, 9, 4], [4571, 6, 8], [4740, 3, 5], [4807, 8, 0], [4823, 9, 4], [4876, 2, 4], [5228, 6, 4], [5246, 7, 2], [5265, 6, 4], [5634, 2, 8], [5642, 1, 4], [5749, 8, 2], [5937, 5, 3], [5955, 3, 8], [5997, 5, 3], [6166, 9, 3], [6168, 9, 3], [6560, 9, 5], [6569, 3, 2], [6571, 9, 3], [6574, 2, 6], [6576, 7, 1], [6597, 0, 9], [6651, 0, 8], [6783, 1, 6], [6796, 2, 7], [8325, 0, 6], [8408, 8, 6], [8527, 4, 9], [9015, 7, 2], [9540, 1, 8], [9587, 9, 4], [9634, 0, 2], [9664, 2, 7], [9679, 6, 4], [9692, 9, 7], [9698, 6, 1], [9729, 5, 6], [9749, 5, 6], [9770, 5, 0], [9982, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "# index_label_prediction_list\n",
    "print(index_label_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2oPIDq11B6Ph"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "iPMaKxiSCCR4",
    "outputId": "d01c3ad5-ff7a-4df2-a9eb-37fcc6fc7ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of index_label_prediction_list =>  120 , false_data_index =>  119\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "false_data_index = np.random.randint(len(index_label_prediction_list))\n",
    "\n",
    "print('len of index_label_prediction_list => ', len(index_label_prediction_list), ', false_data_index => ', false_data_index)\n",
    "\n",
    "mnist_index = index_label_prediction_list[false_data_index][0]\n",
    "label = index_label_prediction_list[false_data_index][1]\n",
    "prediction = index_label_prediction_list[false_data_index][2]\n",
    "\n",
    "title_str = 'index = ' + str(mnist_index) + ' , label = ' + str(label) + ' , prediction = ' + str(prediction)\n",
    "\n",
    "img = test_x_data[mnist_index].reshape(28,28)\n",
    "\n",
    "\n",
    "plt.title(title_str)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gpm6FBXSCcwm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_Example_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
