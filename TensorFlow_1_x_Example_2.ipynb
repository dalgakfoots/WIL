{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2LAKc9SLk92"
   },
   "source": [
    "MNIST 검증 tensorflow 1.x 버전\n",
    "\n",
    "colab 에서 실행하므로 가장 상위에 %tensorflow_version 1.x 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "myejMUL4LaE4",
    "outputId": "4a970423-5b27-45ea-ed0d-7ec234a2b4de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "hii0aBNc3dBu",
    "outputId": "1cec6e49-5320-4819-a2bf-fd06673a90db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-90b8c2e86d09>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\n",
      "train.num =  55000 , test.num =  10000 , validation.num =  5000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"train.num = \", mnist.train.num_examples, \n",
    "      \", test.num = \", mnist.test.num_examples, \n",
    "      \", validation.num = \", mnist.validation.num_examples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bAVGMSD3dB9"
   },
   "source": [
    "#### shape 및 type(mnist) 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "GV7tnrcl3dB-",
    "outputId": "e525ed10-7573-457a-aab1-9a07817e5982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(mnist) =  <class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'> , type(mnist.train.images) =  <class 'numpy.ndarray'> , type(mnist.train.labels) =  <class 'numpy.ndarray'>\n",
      "\n",
      "train image shape =  (55000, 784)\n",
      "train label shape =  (55000, 10)\n",
      "test image shape =  (10000, 784)\n",
      "test label shape =  (10000, 10)\n",
      "\n",
      "train image shape =  (55000, 784)\n",
      "test image shape =  (10000, 784)\n",
      "validation image shape =  (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"type(mnist) = \", type(mnist), \n",
    "      \", type(mnist.train.images) = \", type(mnist.train.images), \n",
    "      \", type(mnist.train.labels) = \", type(mnist.train.labels))\n",
    "\n",
    "print(\"\\ntrain image shape = \", np.shape(mnist.train.images))\n",
    "print(\"train label shape = \", np.shape(mnist.train.labels))\n",
    "print(\"test image shape = \", np.shape(mnist.test.images))\n",
    "print(\"test label shape = \", np.shape(mnist.test.labels))\n",
    "\n",
    "print(\"\\ntrain image shape = \", mnist.train.images.shape)\n",
    "print(\"test image shape = \", mnist.test.images.shape)\n",
    "print(\"validation image shape = \", mnist.validation.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ohhMTGX3dCC"
   },
   "source": [
    "#### train data 정규화 및 label 의 one-hot encoding 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Q9U5KMpD3dCE",
    "outputId": "7e3ee8e0-b661-4ffd-c88c-42b9fb8c210f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of mnist.train.images =  55000\n",
      "\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# 정규화 확인\n",
    "print(\"length of mnist.train.images = \", len(mnist.train.images))\n",
    "\n",
    "for index in range(len(mnist.train.images)):\n",
    "    \n",
    "    min_val = np.min(mnist.train.images[index])\n",
    "    max_val = np.max(mnist.train.images[index])\n",
    "    \n",
    "    if min_val < 0.0:\n",
    "        print(\"min value is \", min_val, \", index = \", index)\n",
    "        break\n",
    "    \n",
    "    if max_val > 1.0:\n",
    "        print(\"max value is \", max_val, \", index = \", index)\n",
    "        break\n",
    "    \n",
    "print(\"\")\n",
    "print(mnist.train.images[0])  # 정규화 확인을 위한 테스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "_CJZqaWY3dCJ",
    "outputId": "83af4745-0f2d-47bb-e9fb-691ca8cda296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of mnist.train.images =  55000\n",
      "\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding 확인\n",
    "print(\"length of mnist.train.images = \", len(mnist.train.labels))\n",
    "\n",
    "for index in range(len(mnist.train.labels)):\n",
    "    \n",
    "    min_val = np.min(mnist.train.labels[index])\n",
    "    max_val = np.max(mnist.train.labels[index])\n",
    "    \n",
    "    if min_val < 0.0:\n",
    "        print(\"min value is \", min_val, \", index = \", index)\n",
    "        break\n",
    "    \n",
    "    if max_val > 1.0:\n",
    "        print(\"max value is \", max_val, \", index = \", index)\n",
    "        break\n",
    "    \n",
    "print(\"\")\n",
    "print(mnist.train.labels[0])  # one-hot encoding 확인을 위한 테스트 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "modsPHJk3dCN"
   },
   "source": [
    "#### Hyper-Parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-A1Zg62n3dCO"
   },
   "outputs": [],
   "source": [
    "# 입력노드, 은닉노드, 출력노드, 학습율, 반복횟수, 배치 개수 등 설정\n",
    "learning_rate = 0.1  # 학습율\n",
    "epochs = 30            # 반복횟수\n",
    "batch_size = 100      # 한번에 입력으로 주어지는 MNIST 개수\n",
    "\n",
    "input_nodes = 784     # 입력노드 개수\n",
    "hidden_nodes = 100    # 은닉노드 개수\n",
    "output_nodes = 10     # 출력노드 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiUHKuBe3dCW"
   },
   "source": [
    "#### 입력과 출력을 위한 플레이스홀더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWEwL-823dCX"
   },
   "outputs": [],
   "source": [
    "# 입력과 출력을 위한 플레이스홀더 정의\n",
    "X = tf.placeholder(tf.float32, [None, input_nodes])  \n",
    "T = tf.placeholder(tf.float32, [None, output_nodes])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4h7965P3dCc"
   },
   "source": [
    "#### 가중치, 바이어스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3h1WGBDF3dCd"
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([input_nodes, hidden_nodes]))  # 은닉층 가중치 노드\n",
    "b2 = tf.Variable(tf.random_normal([hidden_nodes]))               # 은닉층 바이어스 노드\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([hidden_nodes, output_nodes])) # 출력층 가중치 노드\n",
    "b3 = tf.Variable(tf.random_normal([output_nodes]))               # 출력층 바이어스 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C85SQmbt3dCh"
   },
   "outputs": [],
   "source": [
    "Z2 = tf.matmul(X, W2) + b2    # 선형회귀 선형회귀 값 Z2\n",
    "A2 = tf.nn.relu(Z2)           # 은닉층 출력 값 A2, sigmoid 대신 relu 사용\n",
    "\n",
    "# 출력층 선형회귀  값 Z3, 즉 softmax 에 들어가는 입력 값\n",
    "Z3 = logits = tf.matmul(A2, W3) + b3   \n",
    "\n",
    "y = A3 = tf.nn.softmax(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SA4oMwL73dCl"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=T) )\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train = optimizer.minimize(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAtYhqFg3dCp"
   },
   "outputs": [],
   "source": [
    "# batch_size X 10 데이터에 대해 argmax를 통해 행단위로 비교함\n",
    "predicted_val = tf.equal( tf.argmax(A3, 1), tf.argmax(T, 1) )\n",
    "\n",
    "# batch_size X 10 의 True, False 를 1 또는 0 으로 변환\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))\n",
    "\n",
    "# index list 출력\n",
    "accuracy_index = tf.cast(predicted_val, dtype=tf.float32)\n",
    "\n",
    "# 예측값 처리\n",
    "predicted_list = tf.argmax(A3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "yTd24gV23dCt",
    "outputId": "7b337495-3904-436c-a5cb-2d01f5f0ccf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(predicted_val) =  <class 'tensorflow.python.framework.ops.Tensor'> , type(accuracy) =  <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "type(accuracy_index) = <class 'tensorflow.python.framework.ops.Tensor'> , type(predicted_list) =  <class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Type Check\n",
    "print('type(predicted_val) = ', type(predicted_val),  ', type(accuracy) = ', type(accuracy))\n",
    "print('type(accuracy_index) =', type(accuracy_index), ', type(predicted_list) = ', type(predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0msF9q3x3dCx",
    "outputId": "d931562d-12af-4eb0-ca50-cf6b8db9db9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2, W3, b2, b3 before learning\n",
      "=======================================\n",
      "W2_val[1] =  [-1.2939392   0.99423605  0.8549633   0.04300823 -1.1953812  -0.70256305\n",
      "  0.7324872  -0.60870767  0.75441813  1.5365961  -0.9943607  -1.0257955\n",
      "  0.684822   -0.18364082 -0.29557073 -0.02231202  2.1815798   0.1073936\n",
      "  0.19448258 -0.2180766   1.0918636   0.25642022  0.4516054  -0.3421596\n",
      "  0.1238434  -0.16964026  0.06822886  0.4731287  -1.7862161  -1.450282\n",
      " -0.02280569 -1.315163   -1.6674564   0.43834788 -1.1413357   0.7725334\n",
      "  1.1276854  -0.38365442 -0.26385406 -0.47827968 -0.33362553 -0.40954432\n",
      " -0.03588914  2.9946868  -0.724592    0.41539133 -0.20986766  1.0915383\n",
      " -0.5897524  -0.06595538  1.0496141  -0.28803623  1.1088094  -2.0288172\n",
      "  0.8082863  -1.218676   -0.03964697  1.2904036  -0.53940463  1.8456044\n",
      "  1.3422954   1.4260501  -0.18997157  0.0436684  -0.33023983  0.8090994\n",
      "  1.476035   -0.62762964 -1.429708    1.4416537   0.99577236  1.569535\n",
      " -0.42027947  0.36816943  0.5989289   0.00479751  0.49494374 -0.8409527\n",
      "  0.6662874  -1.1277572  -1.8827437  -0.570025    1.0297801  -0.00883795\n",
      "  0.09090433  0.46032706 -0.20129927 -0.5789229  -1.6550928   2.234774\n",
      "  0.02221886  0.6610116   0.9689655  -0.5849981   1.6210666  -1.0167186\n",
      "  1.0681258   0.77394104 -0.70672435 -0.18564792]\n",
      "W3_val[2] =  [-1.0597842  -1.6751475   0.40161234 -0.03020125  0.14532119 -0.9403567\n",
      "  1.2171046   0.92654127 -0.0920416  -0.17976573]\n",
      "b2_val[1] =  2.0768633\n",
      "b3_val[2] =  -0.8766538\n",
      "=======================================\n",
      "epochs =  0 , step =  0 , loss_val =  115.19867\n",
      "epochs =  0 , step =  100 , loss_val =  5.683501\n",
      "epochs =  0 , step =  200 , loss_val =  1.7112782\n",
      "epochs =  0 , step =  300 , loss_val =  2.9747717\n",
      "epochs =  0 , step =  400 , loss_val =  2.313879\n",
      "epochs =  0 , step =  500 , loss_val =  2.363808\n",
      "epochs =  1 , step =  0 , loss_val =  1.6402375\n",
      "epochs =  1 , step =  100 , loss_val =  1.2346528\n",
      "epochs =  1 , step =  200 , loss_val =  3.1575854\n",
      "epochs =  1 , step =  300 , loss_val =  1.6420845\n",
      "epochs =  1 , step =  400 , loss_val =  1.4539963\n",
      "epochs =  1 , step =  500 , loss_val =  0.58349574\n",
      "epochs =  2 , step =  0 , loss_val =  0.75964266\n",
      "epochs =  2 , step =  100 , loss_val =  1.3285099\n",
      "epochs =  2 , step =  200 , loss_val =  0.38103947\n",
      "epochs =  2 , step =  300 , loss_val =  0.4558156\n",
      "epochs =  2 , step =  400 , loss_val =  0.6065685\n",
      "epochs =  2 , step =  500 , loss_val =  0.5662693\n",
      "epochs =  3 , step =  0 , loss_val =  0.7527235\n",
      "epochs =  3 , step =  100 , loss_val =  0.37933242\n",
      "epochs =  3 , step =  200 , loss_val =  1.0297385\n",
      "epochs =  3 , step =  300 , loss_val =  0.48078927\n",
      "epochs =  3 , step =  400 , loss_val =  0.35684514\n",
      "epochs =  3 , step =  500 , loss_val =  0.24822253\n",
      "epochs =  4 , step =  0 , loss_val =  0.5111286\n",
      "epochs =  4 , step =  100 , loss_val =  0.28269562\n",
      "epochs =  4 , step =  200 , loss_val =  0.41078743\n",
      "epochs =  4 , step =  300 , loss_val =  1.0328526\n",
      "epochs =  4 , step =  400 , loss_val =  0.41297382\n",
      "epochs =  4 , step =  500 , loss_val =  0.78630495\n",
      "epochs =  5 , step =  0 , loss_val =  0.40023136\n",
      "epochs =  5 , step =  100 , loss_val =  0.62220377\n",
      "epochs =  5 , step =  200 , loss_val =  0.53092635\n",
      "epochs =  5 , step =  300 , loss_val =  0.31538063\n",
      "epochs =  5 , step =  400 , loss_val =  0.26385945\n",
      "epochs =  5 , step =  500 , loss_val =  0.3934048\n",
      "epochs =  6 , step =  0 , loss_val =  0.58752275\n",
      "epochs =  6 , step =  100 , loss_val =  0.42109603\n",
      "epochs =  6 , step =  200 , loss_val =  0.25533634\n",
      "epochs =  6 , step =  300 , loss_val =  0.35184518\n",
      "epochs =  6 , step =  400 , loss_val =  0.28382075\n",
      "epochs =  6 , step =  500 , loss_val =  0.45668477\n",
      "epochs =  7 , step =  0 , loss_val =  0.44023076\n",
      "epochs =  7 , step =  100 , loss_val =  0.43918416\n",
      "epochs =  7 , step =  200 , loss_val =  0.3739743\n",
      "epochs =  7 , step =  300 , loss_val =  0.22340602\n",
      "epochs =  7 , step =  400 , loss_val =  0.17733753\n",
      "epochs =  7 , step =  500 , loss_val =  0.30427918\n",
      "epochs =  8 , step =  0 , loss_val =  0.32110417\n",
      "epochs =  8 , step =  100 , loss_val =  0.41022488\n",
      "epochs =  8 , step =  200 , loss_val =  0.26669315\n",
      "epochs =  8 , step =  300 , loss_val =  0.31054488\n",
      "epochs =  8 , step =  400 , loss_val =  0.2984864\n",
      "epochs =  8 , step =  500 , loss_val =  0.23227298\n",
      "epochs =  9 , step =  0 , loss_val =  0.34615818\n",
      "epochs =  9 , step =  100 , loss_val =  0.37101996\n",
      "epochs =  9 , step =  200 , loss_val =  0.25940713\n",
      "epochs =  9 , step =  300 , loss_val =  0.097274296\n",
      "epochs =  9 , step =  400 , loss_val =  0.24048527\n",
      "epochs =  9 , step =  500 , loss_val =  0.42013824\n",
      "epochs =  10 , step =  0 , loss_val =  0.13287732\n",
      "epochs =  10 , step =  100 , loss_val =  0.35117194\n",
      "epochs =  10 , step =  200 , loss_val =  0.48258933\n",
      "epochs =  10 , step =  300 , loss_val =  0.20421128\n",
      "epochs =  10 , step =  400 , loss_val =  0.46581414\n",
      "epochs =  10 , step =  500 , loss_val =  0.37144494\n",
      "epochs =  11 , step =  0 , loss_val =  0.17448413\n",
      "epochs =  11 , step =  100 , loss_val =  0.28269574\n",
      "epochs =  11 , step =  200 , loss_val =  0.10858995\n",
      "epochs =  11 , step =  300 , loss_val =  0.28104457\n",
      "epochs =  11 , step =  400 , loss_val =  0.11181183\n",
      "epochs =  11 , step =  500 , loss_val =  0.27366933\n",
      "epochs =  12 , step =  0 , loss_val =  0.12979494\n",
      "epochs =  12 , step =  100 , loss_val =  0.27573705\n",
      "epochs =  12 , step =  200 , loss_val =  0.103660405\n",
      "epochs =  12 , step =  300 , loss_val =  0.41406494\n",
      "epochs =  12 , step =  400 , loss_val =  0.34528816\n",
      "epochs =  12 , step =  500 , loss_val =  0.27145597\n",
      "epochs =  13 , step =  0 , loss_val =  0.35711208\n",
      "epochs =  13 , step =  100 , loss_val =  0.306218\n",
      "epochs =  13 , step =  200 , loss_val =  0.27302867\n",
      "epochs =  13 , step =  300 , loss_val =  0.33630067\n",
      "epochs =  13 , step =  400 , loss_val =  0.24183187\n",
      "epochs =  13 , step =  500 , loss_val =  0.4065992\n",
      "epochs =  14 , step =  0 , loss_val =  0.27798355\n",
      "epochs =  14 , step =  100 , loss_val =  0.2693313\n",
      "epochs =  14 , step =  200 , loss_val =  0.3796613\n",
      "epochs =  14 , step =  300 , loss_val =  0.18861727\n",
      "epochs =  14 , step =  400 , loss_val =  0.14532742\n",
      "epochs =  14 , step =  500 , loss_val =  0.34234166\n",
      "epochs =  15 , step =  0 , loss_val =  0.23524699\n",
      "epochs =  15 , step =  100 , loss_val =  0.19885445\n",
      "epochs =  15 , step =  200 , loss_val =  0.24480984\n",
      "epochs =  15 , step =  300 , loss_val =  0.32575148\n",
      "epochs =  15 , step =  400 , loss_val =  0.15079905\n",
      "epochs =  15 , step =  500 , loss_val =  0.35466355\n",
      "epochs =  16 , step =  0 , loss_val =  0.12938727\n",
      "epochs =  16 , step =  100 , loss_val =  0.1348847\n",
      "epochs =  16 , step =  200 , loss_val =  0.15487504\n",
      "epochs =  16 , step =  300 , loss_val =  0.23337984\n",
      "epochs =  16 , step =  400 , loss_val =  0.20423655\n",
      "epochs =  16 , step =  500 , loss_val =  0.2960809\n",
      "epochs =  17 , step =  0 , loss_val =  0.16963768\n",
      "epochs =  17 , step =  100 , loss_val =  0.12663454\n",
      "epochs =  17 , step =  200 , loss_val =  0.24530345\n",
      "epochs =  17 , step =  300 , loss_val =  0.17354564\n",
      "epochs =  17 , step =  400 , loss_val =  0.23433033\n",
      "epochs =  17 , step =  500 , loss_val =  0.22831039\n",
      "epochs =  18 , step =  0 , loss_val =  0.40014318\n",
      "epochs =  18 , step =  100 , loss_val =  0.056441616\n",
      "epochs =  18 , step =  200 , loss_val =  0.11971757\n",
      "epochs =  18 , step =  300 , loss_val =  0.14493693\n",
      "epochs =  18 , step =  400 , loss_val =  0.21806088\n",
      "epochs =  18 , step =  500 , loss_val =  0.13451338\n",
      "epochs =  19 , step =  0 , loss_val =  0.13688351\n",
      "epochs =  19 , step =  100 , loss_val =  0.3953319\n",
      "epochs =  19 , step =  200 , loss_val =  0.29461378\n",
      "epochs =  19 , step =  300 , loss_val =  0.254232\n",
      "epochs =  19 , step =  400 , loss_val =  0.23521538\n",
      "epochs =  19 , step =  500 , loss_val =  0.30787882\n",
      "epochs =  20 , step =  0 , loss_val =  0.21983638\n",
      "epochs =  20 , step =  100 , loss_val =  0.21870929\n",
      "epochs =  20 , step =  200 , loss_val =  0.20386757\n",
      "epochs =  20 , step =  300 , loss_val =  0.09865604\n",
      "epochs =  20 , step =  400 , loss_val =  0.17107452\n",
      "epochs =  20 , step =  500 , loss_val =  0.24281497\n",
      "epochs =  21 , step =  0 , loss_val =  0.3787853\n",
      "epochs =  21 , step =  100 , loss_val =  0.10457921\n",
      "epochs =  21 , step =  200 , loss_val =  0.564945\n",
      "epochs =  21 , step =  300 , loss_val =  0.14922124\n",
      "epochs =  21 , step =  400 , loss_val =  0.14932887\n",
      "epochs =  21 , step =  500 , loss_val =  0.13161838\n",
      "epochs =  22 , step =  0 , loss_val =  0.081668705\n",
      "epochs =  22 , step =  100 , loss_val =  0.23413704\n",
      "epochs =  22 , step =  200 , loss_val =  0.42329708\n",
      "epochs =  22 , step =  300 , loss_val =  0.2516842\n",
      "epochs =  22 , step =  400 , loss_val =  0.10730944\n",
      "epochs =  22 , step =  500 , loss_val =  0.21402016\n",
      "epochs =  23 , step =  0 , loss_val =  0.22798361\n",
      "epochs =  23 , step =  100 , loss_val =  0.1987566\n",
      "epochs =  23 , step =  200 , loss_val =  0.1892566\n",
      "epochs =  23 , step =  300 , loss_val =  0.16562161\n",
      "epochs =  23 , step =  400 , loss_val =  0.0810807\n",
      "epochs =  23 , step =  500 , loss_val =  0.24705648\n",
      "epochs =  24 , step =  0 , loss_val =  0.23521812\n",
      "epochs =  24 , step =  100 , loss_val =  0.068060145\n",
      "epochs =  24 , step =  200 , loss_val =  0.53351474\n",
      "epochs =  24 , step =  300 , loss_val =  0.23487125\n",
      "epochs =  24 , step =  400 , loss_val =  0.13596758\n",
      "epochs =  24 , step =  500 , loss_val =  0.45505583\n",
      "epochs =  25 , step =  0 , loss_val =  0.07500744\n",
      "epochs =  25 , step =  100 , loss_val =  0.13988763\n",
      "epochs =  25 , step =  200 , loss_val =  0.1349925\n",
      "epochs =  25 , step =  300 , loss_val =  0.09869227\n",
      "epochs =  25 , step =  400 , loss_val =  0.19119193\n",
      "epochs =  25 , step =  500 , loss_val =  0.15074563\n",
      "epochs =  26 , step =  0 , loss_val =  0.20052469\n",
      "epochs =  26 , step =  100 , loss_val =  0.20507403\n",
      "epochs =  26 , step =  200 , loss_val =  0.27953506\n",
      "epochs =  26 , step =  300 , loss_val =  0.124405906\n",
      "epochs =  26 , step =  400 , loss_val =  0.12326939\n",
      "epochs =  26 , step =  500 , loss_val =  0.1709091\n",
      "epochs =  27 , step =  0 , loss_val =  0.27607825\n",
      "epochs =  27 , step =  100 , loss_val =  0.13060993\n",
      "epochs =  27 , step =  200 , loss_val =  0.11809173\n",
      "epochs =  27 , step =  300 , loss_val =  0.1864427\n",
      "epochs =  27 , step =  400 , loss_val =  0.23443753\n",
      "epochs =  27 , step =  500 , loss_val =  0.23490602\n",
      "epochs =  28 , step =  0 , loss_val =  0.16399592\n",
      "epochs =  28 , step =  100 , loss_val =  0.15276118\n",
      "epochs =  28 , step =  200 , loss_val =  0.16349047\n",
      "epochs =  28 , step =  300 , loss_val =  0.12286585\n",
      "epochs =  28 , step =  400 , loss_val =  0.27349803\n",
      "epochs =  28 , step =  500 , loss_val =  0.19113368\n",
      "epochs =  29 , step =  0 , loss_val =  0.34169605\n",
      "epochs =  29 , step =  100 , loss_val =  0.17645434\n",
      "epochs =  29 , step =  200 , loss_val =  0.13787557\n",
      "epochs =  29 , step =  300 , loss_val =  0.04623066\n",
      "epochs =  29 , step =  400 , loss_val =  0.032833207\n",
      "epochs =  29 , step =  500 , loss_val =  0.09954748\n",
      "\n",
      "Elapsed Time =>  0:00:37.182624\n",
      "\n",
      "\n",
      "type(accuracy_val) =  <class 'numpy.float32'> ,  Accuracy =  0.94\n",
      "========================================================\n",
      "type(accuracy_index_val) =  <class 'numpy.ndarray'> , len(accuracy_index_val) =  10000\n",
      "type(predicted_list_val) =  <class 'numpy.ndarray'> , len(predicted_list_val) =  10000\n",
      "\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[7 2 1 ... 4 5 6]\n",
      "len(false_data_index) =  600\n",
      "[8, 33, 38, 92, 96, 119, 126, 151, 167, 217, 241, 247, 259, 266, 290, 318, 321, 324, 340, 352, 367, 381, 391, 445, 448, 449, 464, 478, 483, 507, 511, 515, 551, 565, 582, 583, 591, 610, 619, 628, 646, 659, 667, 684, 685, 691, 694, 740, 774, 810, 829, 830, 844, 877, 882, 896, 900, 924, 938, 939, 947, 950, 951, 956, 959, 965, 969, 975, 1012, 1014, 1044, 1045, 1050, 1078, 1082, 1101, 1107, 1112, 1147, 1156, 1182, 1192, 1194, 1198, 1204, 1226, 1232, 1234, 1247, 1248, 1251, 1256, 1260, 1283, 1289, 1299, 1319, 1326, 1328, 1337, 1393, 1395, 1429, 1433, 1440, 1444, 1465, 1466, 1469, 1494, 1500, 1518, 1522, 1525, 1527, 1530, 1549, 1551, 1553, 1559, 1581, 1584, 1609, 1634, 1644, 1658, 1669, 1671, 1681, 1695, 1709, 1717, 1718, 1737, 1754, 1759, 1774, 1782, 1790, 1813, 1839, 1850, 1855, 1868, 1871, 1878, 1899, 1901, 1909, 1926, 1930, 1941, 1952, 1955, 1969, 1973, 1982, 1987, 2004, 2016, 2024, 2033, 2035, 2040, 2043, 2044, 2052, 2053, 2070, 2073, 2089, 2098, 2109, 2118, 2125, 2129, 2130, 2135, 2161, 2177, 2182, 2186, 2189, 2200, 2221, 2224, 2229, 2237, 2266, 2272, 2286, 2293, 2298, 2325, 2327, 2369, 2371, 2386, 2387, 2393, 2394, 2406, 2422, 2425, 2430, 2432, 2433, 2437, 2447, 2473, 2488, 2528, 2545, 2560, 2570, 2574, 2578, 2607, 2648, 2654, 2656, 2686, 2695, 2698, 2713, 2743, 2760, 2771, 2832, 2836, 2863, 2877, 2896, 2905, 2907, 2915, 2921, 2927, 2939, 2952, 2970, 2986, 2990, 2995, 3001, 3004, 3005, 3060, 3062, 3073, 3102, 3108, 3110, 3112, 3114, 3117, 3136, 3145, 3160, 3189, 3206, 3240, 3246, 3251, 3319, 3330, 3335, 3358, 3369, 3384, 3473, 3503, 3519, 3520, 3525, 3533, 3552, 3558, 3559, 3564, 3567, 3573, 3574, 3597, 3598, 3604, 3662, 3681, 3690, 3710, 3716, 3726, 3751, 3757, 3763, 3767, 3778, 3780, 3795, 3796, 3806, 3808, 3811, 3833, 3836, 3848, 3853, 3855, 3862, 3869, 3893, 3902, 3906, 3941, 3942, 3943, 3952, 3968, 3976, 3984, 3986, 3988, 3994, 3995, 4007, 4052, 4063, 4065, 4072, 4075, 4078, 4093, 4131, 4145, 4156, 4159, 4163, 4176, 4199, 4201, 4205, 4208, 4211, 4230, 4248, 4256, 4271, 4289, 4306, 4314, 4344, 4354, 4355, 4373, 4374, 4377, 4380, 4403, 4425, 4426, 4435, 4437, 4477, 4497, 4498, 4500, 4536, 4548, 4571, 4575, 4594, 4601, 4615, 4630, 4639, 4655, 4671, 4690, 4692, 4711, 4724, 4731, 4735, 4740, 4751, 4761, 4785, 4807, 4814, 4816, 4823, 4837, 4874, 4879, 4880, 4882, 4886, 4899, 4918, 4939, 4950, 4956, 4966, 4978, 4995, 5011, 5067, 5078, 5140, 5165, 5246, 5278, 5283, 5329, 5331, 5380, 5409, 5457, 5569, 5600, 5608, 5611, 5613, 5634, 5642, 5649, 5676, 5678, 5734, 5746, 5749, 5757, 5821, 5835, 5842, 5858, 5887, 5888, 5913, 5922, 5926, 5936, 5937, 5955, 5972, 5973, 5982, 6009, 6030, 6037, 6042, 6043, 6059, 6071, 6081, 6101, 6157, 6166, 6168, 6172, 6174, 6347, 6391, 6424, 6495, 6505, 6511, 6538, 6558, 6568, 6571, 6573, 6574, 6576, 6590, 6592, 6597, 6598, 6632, 6651, 6659, 6725, 6737, 6914, 7121, 7198, 7216, 7235, 7265, 7338, 7432, 7434, 7451, 7454, 7459, 7511, 7565, 7574, 7595, 7656, 7783, 7812, 7842, 7847, 7849, 7856, 7857, 7858, 7886, 7900, 7902, 7915, 7916, 7917, 7920, 7921, 7928, 7945, 7999, 8004, 8020, 8025, 8036, 8069, 8072, 8091, 8094, 8183, 8246, 8272, 8277, 8279, 8288, 8293, 8325, 8339, 8362, 8375, 8406, 8408, 8426, 8444, 8476, 8487, 8504, 8508, 8519, 8530, 8553, 8679, 9009, 9019, 9024, 9036, 9045, 9139, 9168, 9209, 9253, 9372, 9446, 9449, 9482, 9513, 9530, 9538, 9544, 9587, 9624, 9634, 9642, 9643, 9652, 9655, 9664, 9669, 9679, 9692, 9698, 9716, 9719, 9729, 9738, 9741, 9744, 9745, 9749, 9764, 9770, 9779, 9783, 9808, 9847, 9858, 9904, 9905, 9944, 9975, 9976, 9982]\n",
      "========================================================\n",
      "W2, W3, b2, b3 after learning\n",
      "=======================================\n",
      "W2_val[1] =  [-1.2939392   0.99423605  0.8549633   0.04300823 -1.1953812  -0.70256305\n",
      "  0.7324872  -0.60870767  0.75441813  1.5365961  -0.9943607  -1.0257955\n",
      "  0.684822   -0.18364082 -0.29557073 -0.02231202  2.1815798   0.1073936\n",
      "  0.19448258 -0.2180766   1.0918636   0.25642022  0.4516054  -0.3421596\n",
      "  0.1238434  -0.16964026  0.06822886  0.4731287  -1.7862161  -1.450282\n",
      " -0.02280569 -1.315163   -1.6674564   0.43834788 -1.1413357   0.7725334\n",
      "  1.1276854  -0.38365442 -0.26385406 -0.47827968 -0.33362553 -0.40954432\n",
      " -0.03588914  2.9946868  -0.724592    0.41539133 -0.20986766  1.0915383\n",
      " -0.5897524  -0.06595538  1.0496141  -0.28803623  1.1088094  -2.0288172\n",
      "  0.8082863  -1.218676   -0.03964697  1.2904036  -0.53940463  1.8456044\n",
      "  1.3422954   1.4260501  -0.18997157  0.0436684  -0.33023983  0.8090994\n",
      "  1.476035   -0.62762964 -1.429708    1.4416537   0.99577236  1.569535\n",
      " -0.42027947  0.36816943  0.5989289   0.00479751  0.49494374 -0.8409527\n",
      "  0.6662874  -1.1277572  -1.8827437  -0.570025    1.0297801  -0.00883795\n",
      "  0.09090433  0.46032706 -0.20129927 -0.5789229  -1.6550928   2.234774\n",
      "  0.02221886  0.6610116   0.9689655  -0.5849981   1.6210666  -1.0167186\n",
      "  1.0681258   0.77394104 -0.70672435 -0.18564792]\n",
      "W3_val[2] =  [-0.6734183  -0.14765765  0.12036817 -0.04797272 -0.4127996   0.11886792\n",
      "  0.02525391  0.09030977 -0.07209688 -0.28758267]\n",
      "b2_val[1] =  1.6831905\n",
      "b3_val[2] =  -1.6646483\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "with  tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())  # 변수 노드(tf.Variable) 초기화\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(epochs):    # 50 번 반복수행\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples / batch_size)  # 55,000 / 100\n",
    "\n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "      \n",
    "            loss_val, _ = sess.run([loss, train], feed_dict={X: batch_x_data, T: batch_t_data})    \n",
    "        \n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)             \n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Elapsed Time => \", end_time-start_time)\n",
    "    print(\"\")    \n",
    "    \n",
    "    # Accuracy 확인\n",
    "    test_x_data = mnist.test.images    # 10000 X 784\n",
    "    test_t_data = mnist.test.labels    # 10000 X 10\n",
    "    \n",
    "    accuracy_val = sess.run(accuracy, feed_dict={X: test_x_data, T: test_t_data})\n",
    "    \n",
    "    print('\\ntype(accuracy_val) = ', type(accuracy_val), ',  Accuracy = ', accuracy_val)\n",
    "\n",
    "    accuracy_index_val = sess.run(accuracy_index, feed_dict={X: test_x_data, T: test_t_data})\n",
    "    predicted_list_val = sess.run(predicted_list, feed_dict={X: test_x_data})\n",
    "\n",
    "    print('========================================================')\n",
    "    print('type(accuracy_index_val) = ', type(accuracy_index_val), ', len(accuracy_index_val) = ', len(accuracy_index_val))\n",
    "    print('type(predicted_list_val) = ', type(predicted_list_val), ', len(predicted_list_val) = ', len(predicted_list_val))\n",
    "    print('')\n",
    "    print(accuracy_index_val)\n",
    "    print(predicted_list_val)\n",
    "\n",
    "    false_data_index  = [ index for index, val in enumerate(accuracy_index_val) if val == 0 ]\n",
    "    print('len(false_data_index) = ', len(false_data_index))\n",
    "    print(false_data_index)\n",
    "    print('========================================================')\n",
    "\n",
    "    print('W2, W3, b2, b3 after learning')\n",
    "    print('=======================================')\n",
    "    W2_val, W3_val, b2_val, b3_val = sess.run([W2, W3, b2, b3])\n",
    "    print('W2_val[1] = ' , W2_val[1])\n",
    "    print('W3_val[2] = ' , W3_val[2])\n",
    "    print('b2_val[1] = ' , b2_val[1])\n",
    "    print('b3_val[2] = ' , b3_val[2])\n",
    "    print('=======================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3Yz46CcplnA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlow_1_x_Example_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
