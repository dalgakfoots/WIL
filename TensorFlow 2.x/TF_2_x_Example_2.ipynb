{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_2.x_Example_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmSiuzrH79Rq",
        "colab_type": "code",
        "outputId": "3df64050-61c6-4e97-8efd-dbe5a5f856a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKOCZzEG8KFI",
        "colab_type": "code",
        "outputId": "413d0a56-20cd-490f-b95e-03065098e4df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Google Drive 에 저장되어 있는 diabetes,csv 파일을 읽어들이기 위해서\n",
        "# Colab 의 /content/gdrive/ 에 Google Drive 마운트 시킴\n",
        "\n",
        "from google.colab import drive   # colab 사용시 mount 하기 위한 용도. local 에서는 불필요\n",
        "\n",
        "drive.mount('/content/gdrive/')  # colab 사용시 mount 하기 위한 용도. local 에서는 불필요"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ji_Hyro8PUl",
        "colab_type": "code",
        "outputId": "ef30c246-4834-4123-df84-2c43d0d39641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 마운트된 Google Drive 의 working directory 이동\n",
        "\n",
        "import os\n",
        "\n",
        "# TensorFlow 2.x working directory\n",
        "working_dir = 'tensorflow_2.x_working_dir'\n",
        "\n",
        "# Google Drive 에서 Colab Default Directory\n",
        "colab_default_dir = '/content/gdrive/My Drive/Colab Notebooks/'\n",
        "\n",
        "original_dir = os.getcwd()  # save current dir\n",
        "\n",
        "try:\n",
        "\n",
        "    os.chdir(colab_default_dir)\n",
        "\n",
        "    if not os.path.exists(working_dir):\n",
        "\n",
        "        os.mkdir(working_dir)\n",
        "\n",
        "    os.chdir(working_dir)  # change working dir\n",
        "\n",
        "    print('current dir = ', os.getcwd())\n",
        "\n",
        "except Exception as err:\n",
        "\n",
        "    # 원래의 dir 로 복귀\n",
        "    os.chdir(original_dir)\n",
        "\n",
        "    print(str(err))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current dir =  /content/gdrive/My Drive/Colab Notebooks/tensorflow_2.x_working_dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_1JX_S-8YrX",
        "colab_type": "code",
        "outputId": "40b415fa-5c31-46d6-ceca-05a37e9bdf20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# loadtxt() 이용해서 diabetes.csv 읽어들임\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "\n",
        "    loaded_data = np.loadtxt('./diabetes.csv', delimiter=',')\n",
        "\n",
        "    x_data = loaded_data[ :, 0:-1]\n",
        "    t_data = loaded_data[ :, [-1]]\n",
        "\n",
        "    print(\"x_data.shape = \", x_data.shape)\n",
        "    print(\"t_data.shape = \", t_data.shape)\n",
        "\n",
        "except Exception as err:\n",
        "\n",
        "    print(str(err))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_data.shape =  (759, 8)\n",
            "t_data.shape =  (759, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezD7UAJ3cjQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 은닉층이 없는, 즉 Logistic Regression 을 keras 이용하여 생성\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape=(x_data.shape[1], )))    # 노드 8개인 입력층 생성\n",
        "\n",
        "model.add(Dense(t_data.shape[1], activation='sigmoid'))  # 노드 1개인 출력층 생성"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In-lM-Rd3W76",
        "colab_type": "code",
        "outputId": "75050ea7-9c94-4385-caf8-977d33188ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "model.summary()\n",
        "\n",
        "print(model.input, model.output)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 9\n",
            "Trainable params: 9\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Tensor(\"flatten_input:0\", shape=(None, 8), dtype=float32) Tensor(\"dense/Identity:0\", shape=(None, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3q03Vt2AXFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습을 위한 optimizer, 손실함수 loss 정의\n",
        "\n",
        "model.compile(optimizer=SGD(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_WRpBwEAsIt",
        "colab_type": "code",
        "outputId": "f9bf7b58-3432-48e5-f71f-49010803a90b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "hist = model.fit(x_data, t_data, epochs=500, validation_split=0.25)\n",
        "\n",
        "end_time = datetime.now()\n",
        "\n",
        "print('\\nElapsed Time => ', end_time - start_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.7783 - accuracy: 0.3620 - val_loss: 0.7627 - val_accuracy: 0.4000\n",
            "Epoch 2/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7652 - accuracy: 0.3884 - val_loss: 0.7523 - val_accuracy: 0.4684\n",
            "Epoch 3/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7539 - accuracy: 0.4112 - val_loss: 0.7435 - val_accuracy: 0.4947\n",
            "Epoch 4/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7442 - accuracy: 0.4376 - val_loss: 0.7360 - val_accuracy: 0.4947\n",
            "Epoch 5/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7359 - accuracy: 0.4798 - val_loss: 0.7296 - val_accuracy: 0.5105\n",
            "Epoch 6/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7288 - accuracy: 0.5185 - val_loss: 0.7240 - val_accuracy: 0.5368\n",
            "Epoch 7/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7223 - accuracy: 0.5448 - val_loss: 0.7191 - val_accuracy: 0.5474\n",
            "Epoch 8/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7168 - accuracy: 0.5589 - val_loss: 0.7147 - val_accuracy: 0.5632\n",
            "Epoch 9/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7118 - accuracy: 0.5694 - val_loss: 0.7107 - val_accuracy: 0.5789\n",
            "Epoch 10/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7072 - accuracy: 0.5870 - val_loss: 0.7071 - val_accuracy: 0.5947\n",
            "Epoch 11/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7032 - accuracy: 0.5993 - val_loss: 0.7039 - val_accuracy: 0.6053\n",
            "Epoch 12/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6994 - accuracy: 0.6116 - val_loss: 0.7009 - val_accuracy: 0.6053\n",
            "Epoch 13/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6960 - accuracy: 0.6169 - val_loss: 0.6980 - val_accuracy: 0.6105\n",
            "Epoch 14/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.6239 - val_loss: 0.6954 - val_accuracy: 0.6158\n",
            "Epoch 15/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.6327 - val_loss: 0.6929 - val_accuracy: 0.6158\n",
            "Epoch 16/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6871 - accuracy: 0.6380 - val_loss: 0.6905 - val_accuracy: 0.6158\n",
            "Epoch 17/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.6397 - val_loss: 0.6882 - val_accuracy: 0.6158\n",
            "Epoch 18/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6820 - accuracy: 0.6380 - val_loss: 0.6860 - val_accuracy: 0.6211\n",
            "Epoch 19/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6796 - accuracy: 0.6397 - val_loss: 0.6839 - val_accuracy: 0.6211\n",
            "Epoch 20/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6773 - accuracy: 0.6397 - val_loss: 0.6818 - val_accuracy: 0.6211\n",
            "Epoch 21/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6751 - accuracy: 0.6450 - val_loss: 0.6798 - val_accuracy: 0.6263\n",
            "Epoch 22/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6730 - accuracy: 0.6450 - val_loss: 0.6778 - val_accuracy: 0.6263\n",
            "Epoch 23/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6709 - accuracy: 0.6467 - val_loss: 0.6758 - val_accuracy: 0.6263\n",
            "Epoch 24/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6689 - accuracy: 0.6485 - val_loss: 0.6739 - val_accuracy: 0.6263\n",
            "Epoch 25/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6669 - accuracy: 0.6503 - val_loss: 0.6720 - val_accuracy: 0.6263\n",
            "Epoch 26/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.6650 - accuracy: 0.6520 - val_loss: 0.6701 - val_accuracy: 0.6263\n",
            "Epoch 27/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.6520 - val_loss: 0.6683 - val_accuracy: 0.6263\n",
            "Epoch 28/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6520 - val_loss: 0.6665 - val_accuracy: 0.6263\n",
            "Epoch 29/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6538 - val_loss: 0.6647 - val_accuracy: 0.6263\n",
            "Epoch 30/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6578 - accuracy: 0.6573 - val_loss: 0.6629 - val_accuracy: 0.6263\n",
            "Epoch 31/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6560 - accuracy: 0.6573 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 32/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6543 - accuracy: 0.6573 - val_loss: 0.6594 - val_accuracy: 0.6263\n",
            "Epoch 33/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6526 - accuracy: 0.6573 - val_loss: 0.6576 - val_accuracy: 0.6263\n",
            "Epoch 34/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6510 - accuracy: 0.6573 - val_loss: 0.6559 - val_accuracy: 0.6263\n",
            "Epoch 35/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6494 - accuracy: 0.6573 - val_loss: 0.6542 - val_accuracy: 0.6263\n",
            "Epoch 36/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6477 - accuracy: 0.6573 - val_loss: 0.6525 - val_accuracy: 0.6316\n",
            "Epoch 37/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6461 - accuracy: 0.6573 - val_loss: 0.6509 - val_accuracy: 0.6316\n",
            "Epoch 38/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 0.6573 - val_loss: 0.6492 - val_accuracy: 0.6316\n",
            "Epoch 39/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6430 - accuracy: 0.6573 - val_loss: 0.6475 - val_accuracy: 0.6316\n",
            "Epoch 40/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6415 - accuracy: 0.6573 - val_loss: 0.6459 - val_accuracy: 0.6316\n",
            "Epoch 41/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6400 - accuracy: 0.6573 - val_loss: 0.6443 - val_accuracy: 0.6316\n",
            "Epoch 42/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6385 - accuracy: 0.6573 - val_loss: 0.6427 - val_accuracy: 0.6316\n",
            "Epoch 43/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6370 - accuracy: 0.6573 - val_loss: 0.6412 - val_accuracy: 0.6316\n",
            "Epoch 44/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6356 - accuracy: 0.6555 - val_loss: 0.6396 - val_accuracy: 0.6316\n",
            "Epoch 45/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6341 - accuracy: 0.6555 - val_loss: 0.6381 - val_accuracy: 0.6368\n",
            "Epoch 46/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6327 - accuracy: 0.6555 - val_loss: 0.6366 - val_accuracy: 0.6368\n",
            "Epoch 47/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6313 - accuracy: 0.6555 - val_loss: 0.6351 - val_accuracy: 0.6368\n",
            "Epoch 48/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6299 - accuracy: 0.6555 - val_loss: 0.6336 - val_accuracy: 0.6368\n",
            "Epoch 49/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6285 - accuracy: 0.6573 - val_loss: 0.6320 - val_accuracy: 0.6368\n",
            "Epoch 50/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6272 - accuracy: 0.6573 - val_loss: 0.6306 - val_accuracy: 0.6368\n",
            "Epoch 51/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6259 - accuracy: 0.6573 - val_loss: 0.6292 - val_accuracy: 0.6368\n",
            "Epoch 52/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6245 - accuracy: 0.6573 - val_loss: 0.6277 - val_accuracy: 0.6368\n",
            "Epoch 53/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6232 - accuracy: 0.6573 - val_loss: 0.6263 - val_accuracy: 0.6368\n",
            "Epoch 54/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6219 - accuracy: 0.6573 - val_loss: 0.6249 - val_accuracy: 0.6368\n",
            "Epoch 55/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6206 - accuracy: 0.6573 - val_loss: 0.6235 - val_accuracy: 0.6368\n",
            "Epoch 56/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6194 - accuracy: 0.6591 - val_loss: 0.6222 - val_accuracy: 0.6368\n",
            "Epoch 57/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6182 - accuracy: 0.6573 - val_loss: 0.6208 - val_accuracy: 0.6368\n",
            "Epoch 58/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6169 - accuracy: 0.6573 - val_loss: 0.6194 - val_accuracy: 0.6368\n",
            "Epoch 59/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6157 - accuracy: 0.6555 - val_loss: 0.6181 - val_accuracy: 0.6368\n",
            "Epoch 60/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6145 - accuracy: 0.6555 - val_loss: 0.6168 - val_accuracy: 0.6368\n",
            "Epoch 61/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6133 - accuracy: 0.6591 - val_loss: 0.6155 - val_accuracy: 0.6368\n",
            "Epoch 62/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6121 - accuracy: 0.6573 - val_loss: 0.6142 - val_accuracy: 0.6368\n",
            "Epoch 63/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6109 - accuracy: 0.6591 - val_loss: 0.6129 - val_accuracy: 0.6368\n",
            "Epoch 64/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6098 - accuracy: 0.6591 - val_loss: 0.6117 - val_accuracy: 0.6368\n",
            "Epoch 65/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6086 - accuracy: 0.6591 - val_loss: 0.6104 - val_accuracy: 0.6368\n",
            "Epoch 66/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6075 - accuracy: 0.6573 - val_loss: 0.6092 - val_accuracy: 0.6368\n",
            "Epoch 67/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6064 - accuracy: 0.6591 - val_loss: 0.6079 - val_accuracy: 0.6368\n",
            "Epoch 68/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6053 - accuracy: 0.6573 - val_loss: 0.6067 - val_accuracy: 0.6368\n",
            "Epoch 69/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6042 - accuracy: 0.6573 - val_loss: 0.6055 - val_accuracy: 0.6316\n",
            "Epoch 70/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6032 - accuracy: 0.6591 - val_loss: 0.6043 - val_accuracy: 0.6316\n",
            "Epoch 71/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6021 - accuracy: 0.6626 - val_loss: 0.6032 - val_accuracy: 0.6316\n",
            "Epoch 72/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6010 - accuracy: 0.6626 - val_loss: 0.6020 - val_accuracy: 0.6316\n",
            "Epoch 73/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6000 - accuracy: 0.6626 - val_loss: 0.6009 - val_accuracy: 0.6316\n",
            "Epoch 74/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5990 - accuracy: 0.6626 - val_loss: 0.5997 - val_accuracy: 0.6316\n",
            "Epoch 75/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5980 - accuracy: 0.6626 - val_loss: 0.5986 - val_accuracy: 0.6316\n",
            "Epoch 76/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5969 - accuracy: 0.6626 - val_loss: 0.5975 - val_accuracy: 0.6316\n",
            "Epoch 77/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5959 - accuracy: 0.6643 - val_loss: 0.5964 - val_accuracy: 0.6316\n",
            "Epoch 78/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5949 - accuracy: 0.6661 - val_loss: 0.5953 - val_accuracy: 0.6316\n",
            "Epoch 79/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5940 - accuracy: 0.6696 - val_loss: 0.5942 - val_accuracy: 0.6368\n",
            "Epoch 80/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5931 - accuracy: 0.6696 - val_loss: 0.5931 - val_accuracy: 0.6368\n",
            "Epoch 81/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5921 - accuracy: 0.6696 - val_loss: 0.5921 - val_accuracy: 0.6368\n",
            "Epoch 82/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5911 - accuracy: 0.6696 - val_loss: 0.5911 - val_accuracy: 0.6368\n",
            "Epoch 83/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5902 - accuracy: 0.6696 - val_loss: 0.5900 - val_accuracy: 0.6368\n",
            "Epoch 84/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5893 - accuracy: 0.6696 - val_loss: 0.5890 - val_accuracy: 0.6368\n",
            "Epoch 85/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5884 - accuracy: 0.6696 - val_loss: 0.5880 - val_accuracy: 0.6368\n",
            "Epoch 86/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5875 - accuracy: 0.6696 - val_loss: 0.5870 - val_accuracy: 0.6368\n",
            "Epoch 87/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5866 - accuracy: 0.6714 - val_loss: 0.5860 - val_accuracy: 0.6368\n",
            "Epoch 88/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5858 - accuracy: 0.6714 - val_loss: 0.5850 - val_accuracy: 0.6368\n",
            "Epoch 89/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.6696 - val_loss: 0.5840 - val_accuracy: 0.6368\n",
            "Epoch 90/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5840 - accuracy: 0.6714 - val_loss: 0.5830 - val_accuracy: 0.6421\n",
            "Epoch 91/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5832 - accuracy: 0.6714 - val_loss: 0.5821 - val_accuracy: 0.6474\n",
            "Epoch 92/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5822 - accuracy: 0.6731 - val_loss: 0.5811 - val_accuracy: 0.6474\n",
            "Epoch 93/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5814 - accuracy: 0.6749 - val_loss: 0.5802 - val_accuracy: 0.6474\n",
            "Epoch 94/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5806 - accuracy: 0.6749 - val_loss: 0.5793 - val_accuracy: 0.6474\n",
            "Epoch 95/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5798 - accuracy: 0.6766 - val_loss: 0.5784 - val_accuracy: 0.6474\n",
            "Epoch 96/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5790 - accuracy: 0.6766 - val_loss: 0.5775 - val_accuracy: 0.6474\n",
            "Epoch 97/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5782 - accuracy: 0.6766 - val_loss: 0.5766 - val_accuracy: 0.6474\n",
            "Epoch 98/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5774 - accuracy: 0.6766 - val_loss: 0.5757 - val_accuracy: 0.6474\n",
            "Epoch 99/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5766 - accuracy: 0.6766 - val_loss: 0.5748 - val_accuracy: 0.6474\n",
            "Epoch 100/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5758 - accuracy: 0.6766 - val_loss: 0.5739 - val_accuracy: 0.6474\n",
            "Epoch 101/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5751 - accuracy: 0.6784 - val_loss: 0.5731 - val_accuracy: 0.6474\n",
            "Epoch 102/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5743 - accuracy: 0.6784 - val_loss: 0.5722 - val_accuracy: 0.6474\n",
            "Epoch 103/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5736 - accuracy: 0.6784 - val_loss: 0.5714 - val_accuracy: 0.6474\n",
            "Epoch 104/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5729 - accuracy: 0.6784 - val_loss: 0.5706 - val_accuracy: 0.6474\n",
            "Epoch 105/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5721 - accuracy: 0.6819 - val_loss: 0.5697 - val_accuracy: 0.6474\n",
            "Epoch 106/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5714 - accuracy: 0.6819 - val_loss: 0.5689 - val_accuracy: 0.6474\n",
            "Epoch 107/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5707 - accuracy: 0.6819 - val_loss: 0.5681 - val_accuracy: 0.6474\n",
            "Epoch 108/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5699 - accuracy: 0.6819 - val_loss: 0.5673 - val_accuracy: 0.6474\n",
            "Epoch 109/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5693 - accuracy: 0.6837 - val_loss: 0.5666 - val_accuracy: 0.6526\n",
            "Epoch 110/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5685 - accuracy: 0.6872 - val_loss: 0.5658 - val_accuracy: 0.6526\n",
            "Epoch 111/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5679 - accuracy: 0.6872 - val_loss: 0.5650 - val_accuracy: 0.6526\n",
            "Epoch 112/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5672 - accuracy: 0.6872 - val_loss: 0.5642 - val_accuracy: 0.6526\n",
            "Epoch 113/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5665 - accuracy: 0.6872 - val_loss: 0.5635 - val_accuracy: 0.6474\n",
            "Epoch 114/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5659 - accuracy: 0.6889 - val_loss: 0.5627 - val_accuracy: 0.6474\n",
            "Epoch 115/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5652 - accuracy: 0.6907 - val_loss: 0.5619 - val_accuracy: 0.6474\n",
            "Epoch 116/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5645 - accuracy: 0.6924 - val_loss: 0.5612 - val_accuracy: 0.6474\n",
            "Epoch 117/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5639 - accuracy: 0.6942 - val_loss: 0.5605 - val_accuracy: 0.6474\n",
            "Epoch 118/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5633 - accuracy: 0.6924 - val_loss: 0.5597 - val_accuracy: 0.6526\n",
            "Epoch 119/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5626 - accuracy: 0.6942 - val_loss: 0.5590 - val_accuracy: 0.6526\n",
            "Epoch 120/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5620 - accuracy: 0.6977 - val_loss: 0.5583 - val_accuracy: 0.6579\n",
            "Epoch 121/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5614 - accuracy: 0.6995 - val_loss: 0.5576 - val_accuracy: 0.6579\n",
            "Epoch 122/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5607 - accuracy: 0.6995 - val_loss: 0.5569 - val_accuracy: 0.6579\n",
            "Epoch 123/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5602 - accuracy: 0.7012 - val_loss: 0.5562 - val_accuracy: 0.6579\n",
            "Epoch 124/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7030 - val_loss: 0.5555 - val_accuracy: 0.6632\n",
            "Epoch 125/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5589 - accuracy: 0.7012 - val_loss: 0.5548 - val_accuracy: 0.6579\n",
            "Epoch 126/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5583 - accuracy: 0.7012 - val_loss: 0.5542 - val_accuracy: 0.6579\n",
            "Epoch 127/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5578 - accuracy: 0.7012 - val_loss: 0.5535 - val_accuracy: 0.6579\n",
            "Epoch 128/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5572 - accuracy: 0.7030 - val_loss: 0.5528 - val_accuracy: 0.6632\n",
            "Epoch 129/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5566 - accuracy: 0.7083 - val_loss: 0.5522 - val_accuracy: 0.6632\n",
            "Epoch 130/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5561 - accuracy: 0.7083 - val_loss: 0.5515 - val_accuracy: 0.6632\n",
            "Epoch 131/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5555 - accuracy: 0.7100 - val_loss: 0.5509 - val_accuracy: 0.6632\n",
            "Epoch 132/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5549 - accuracy: 0.7100 - val_loss: 0.5503 - val_accuracy: 0.6632\n",
            "Epoch 133/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5544 - accuracy: 0.7135 - val_loss: 0.5496 - val_accuracy: 0.6684\n",
            "Epoch 134/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5539 - accuracy: 0.7188 - val_loss: 0.5490 - val_accuracy: 0.6684\n",
            "Epoch 135/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.7188 - val_loss: 0.5484 - val_accuracy: 0.6684\n",
            "Epoch 136/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5528 - accuracy: 0.7188 - val_loss: 0.5478 - val_accuracy: 0.6737\n",
            "Epoch 137/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5523 - accuracy: 0.7188 - val_loss: 0.5471 - val_accuracy: 0.6789\n",
            "Epoch 138/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5517 - accuracy: 0.7206 - val_loss: 0.5465 - val_accuracy: 0.6789\n",
            "Epoch 139/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5512 - accuracy: 0.7223 - val_loss: 0.5459 - val_accuracy: 0.6789\n",
            "Epoch 140/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5507 - accuracy: 0.7223 - val_loss: 0.5454 - val_accuracy: 0.6789\n",
            "Epoch 141/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5502 - accuracy: 0.7241 - val_loss: 0.5448 - val_accuracy: 0.6789\n",
            "Epoch 142/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5497 - accuracy: 0.7241 - val_loss: 0.5442 - val_accuracy: 0.6789\n",
            "Epoch 143/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5492 - accuracy: 0.7241 - val_loss: 0.5436 - val_accuracy: 0.6789\n",
            "Epoch 144/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5487 - accuracy: 0.7258 - val_loss: 0.5431 - val_accuracy: 0.6842\n",
            "Epoch 145/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5482 - accuracy: 0.7258 - val_loss: 0.5425 - val_accuracy: 0.6842\n",
            "Epoch 146/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5477 - accuracy: 0.7258 - val_loss: 0.5420 - val_accuracy: 0.6842\n",
            "Epoch 147/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5473 - accuracy: 0.7258 - val_loss: 0.5414 - val_accuracy: 0.6842\n",
            "Epoch 148/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5467 - accuracy: 0.7276 - val_loss: 0.5409 - val_accuracy: 0.6842\n",
            "Epoch 149/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5463 - accuracy: 0.7276 - val_loss: 0.5403 - val_accuracy: 0.6842\n",
            "Epoch 150/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5458 - accuracy: 0.7258 - val_loss: 0.5398 - val_accuracy: 0.6895\n",
            "Epoch 151/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5454 - accuracy: 0.7276 - val_loss: 0.5393 - val_accuracy: 0.6895\n",
            "Epoch 152/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5449 - accuracy: 0.7293 - val_loss: 0.5387 - val_accuracy: 0.6895\n",
            "Epoch 153/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5444 - accuracy: 0.7293 - val_loss: 0.5382 - val_accuracy: 0.6895\n",
            "Epoch 154/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5441 - accuracy: 0.7293 - val_loss: 0.5377 - val_accuracy: 0.6895\n",
            "Epoch 155/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5435 - accuracy: 0.7293 - val_loss: 0.5372 - val_accuracy: 0.7000\n",
            "Epoch 156/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5431 - accuracy: 0.7293 - val_loss: 0.5367 - val_accuracy: 0.7053\n",
            "Epoch 157/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5426 - accuracy: 0.7293 - val_loss: 0.5362 - val_accuracy: 0.7053\n",
            "Epoch 158/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5422 - accuracy: 0.7293 - val_loss: 0.5357 - val_accuracy: 0.7053\n",
            "Epoch 159/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5418 - accuracy: 0.7293 - val_loss: 0.5352 - val_accuracy: 0.7053\n",
            "Epoch 160/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5414 - accuracy: 0.7293 - val_loss: 0.5347 - val_accuracy: 0.7053\n",
            "Epoch 161/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5410 - accuracy: 0.7293 - val_loss: 0.5342 - val_accuracy: 0.7053\n",
            "Epoch 162/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5406 - accuracy: 0.7293 - val_loss: 0.5337 - val_accuracy: 0.7053\n",
            "Epoch 163/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5402 - accuracy: 0.7311 - val_loss: 0.5332 - val_accuracy: 0.7105\n",
            "Epoch 164/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5397 - accuracy: 0.7329 - val_loss: 0.5328 - val_accuracy: 0.7105\n",
            "Epoch 165/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5393 - accuracy: 0.7329 - val_loss: 0.5323 - val_accuracy: 0.7105\n",
            "Epoch 166/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5389 - accuracy: 0.7311 - val_loss: 0.5318 - val_accuracy: 0.7105\n",
            "Epoch 167/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5385 - accuracy: 0.7311 - val_loss: 0.5313 - val_accuracy: 0.7105\n",
            "Epoch 168/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5381 - accuracy: 0.7311 - val_loss: 0.5309 - val_accuracy: 0.7105\n",
            "Epoch 169/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5377 - accuracy: 0.7311 - val_loss: 0.5304 - val_accuracy: 0.7105\n",
            "Epoch 170/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5374 - accuracy: 0.7311 - val_loss: 0.5300 - val_accuracy: 0.7105\n",
            "Epoch 171/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5370 - accuracy: 0.7311 - val_loss: 0.5295 - val_accuracy: 0.7158\n",
            "Epoch 172/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5366 - accuracy: 0.7311 - val_loss: 0.5291 - val_accuracy: 0.7158\n",
            "Epoch 173/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5362 - accuracy: 0.7329 - val_loss: 0.5287 - val_accuracy: 0.7211\n",
            "Epoch 174/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5358 - accuracy: 0.7364 - val_loss: 0.5282 - val_accuracy: 0.7211\n",
            "Epoch 175/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5355 - accuracy: 0.7364 - val_loss: 0.5278 - val_accuracy: 0.7263\n",
            "Epoch 176/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5351 - accuracy: 0.7364 - val_loss: 0.5274 - val_accuracy: 0.7263\n",
            "Epoch 177/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5347 - accuracy: 0.7364 - val_loss: 0.5269 - val_accuracy: 0.7263\n",
            "Epoch 178/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7364 - val_loss: 0.5265 - val_accuracy: 0.7263\n",
            "Epoch 179/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5340 - accuracy: 0.7381 - val_loss: 0.5261 - val_accuracy: 0.7263\n",
            "Epoch 180/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5337 - accuracy: 0.7381 - val_loss: 0.5257 - val_accuracy: 0.7263\n",
            "Epoch 181/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5333 - accuracy: 0.7381 - val_loss: 0.5253 - val_accuracy: 0.7263\n",
            "Epoch 182/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5330 - accuracy: 0.7399 - val_loss: 0.5249 - val_accuracy: 0.7263\n",
            "Epoch 183/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5327 - accuracy: 0.7399 - val_loss: 0.5245 - val_accuracy: 0.7263\n",
            "Epoch 184/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5323 - accuracy: 0.7399 - val_loss: 0.5241 - val_accuracy: 0.7263\n",
            "Epoch 185/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5320 - accuracy: 0.7399 - val_loss: 0.5237 - val_accuracy: 0.7263\n",
            "Epoch 186/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5316 - accuracy: 0.7417 - val_loss: 0.5233 - val_accuracy: 0.7263\n",
            "Epoch 187/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5313 - accuracy: 0.7417 - val_loss: 0.5229 - val_accuracy: 0.7263\n",
            "Epoch 188/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5310 - accuracy: 0.7434 - val_loss: 0.5225 - val_accuracy: 0.7263\n",
            "Epoch 189/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5307 - accuracy: 0.7469 - val_loss: 0.5221 - val_accuracy: 0.7263\n",
            "Epoch 190/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5303 - accuracy: 0.7469 - val_loss: 0.5217 - val_accuracy: 0.7368\n",
            "Epoch 191/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5300 - accuracy: 0.7469 - val_loss: 0.5214 - val_accuracy: 0.7368\n",
            "Epoch 192/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5297 - accuracy: 0.7487 - val_loss: 0.5210 - val_accuracy: 0.7368\n",
            "Epoch 193/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5294 - accuracy: 0.7487 - val_loss: 0.5206 - val_accuracy: 0.7368\n",
            "Epoch 194/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5291 - accuracy: 0.7487 - val_loss: 0.5202 - val_accuracy: 0.7368\n",
            "Epoch 195/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5287 - accuracy: 0.7487 - val_loss: 0.5199 - val_accuracy: 0.7368\n",
            "Epoch 196/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5284 - accuracy: 0.7504 - val_loss: 0.5195 - val_accuracy: 0.7368\n",
            "Epoch 197/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5281 - accuracy: 0.7504 - val_loss: 0.5191 - val_accuracy: 0.7368\n",
            "Epoch 198/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5278 - accuracy: 0.7504 - val_loss: 0.5188 - val_accuracy: 0.7368\n",
            "Epoch 199/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5275 - accuracy: 0.7522 - val_loss: 0.5184 - val_accuracy: 0.7368\n",
            "Epoch 200/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5273 - accuracy: 0.7522 - val_loss: 0.5181 - val_accuracy: 0.7316\n",
            "Epoch 201/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5270 - accuracy: 0.7540 - val_loss: 0.5177 - val_accuracy: 0.7316\n",
            "Epoch 202/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5267 - accuracy: 0.7540 - val_loss: 0.5173 - val_accuracy: 0.7368\n",
            "Epoch 203/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5264 - accuracy: 0.7522 - val_loss: 0.5170 - val_accuracy: 0.7368\n",
            "Epoch 204/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5261 - accuracy: 0.7522 - val_loss: 0.5167 - val_accuracy: 0.7368\n",
            "Epoch 205/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5258 - accuracy: 0.7522 - val_loss: 0.5163 - val_accuracy: 0.7368\n",
            "Epoch 206/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5255 - accuracy: 0.7522 - val_loss: 0.5160 - val_accuracy: 0.7368\n",
            "Epoch 207/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5252 - accuracy: 0.7522 - val_loss: 0.5156 - val_accuracy: 0.7368\n",
            "Epoch 208/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5249 - accuracy: 0.7522 - val_loss: 0.5153 - val_accuracy: 0.7368\n",
            "Epoch 209/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5247 - accuracy: 0.7540 - val_loss: 0.5150 - val_accuracy: 0.7368\n",
            "Epoch 210/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5244 - accuracy: 0.7540 - val_loss: 0.5147 - val_accuracy: 0.7368\n",
            "Epoch 211/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5241 - accuracy: 0.7540 - val_loss: 0.5143 - val_accuracy: 0.7368\n",
            "Epoch 212/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5238 - accuracy: 0.7557 - val_loss: 0.5140 - val_accuracy: 0.7368\n",
            "Epoch 213/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5236 - accuracy: 0.7557 - val_loss: 0.5137 - val_accuracy: 0.7368\n",
            "Epoch 214/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5233 - accuracy: 0.7557 - val_loss: 0.5134 - val_accuracy: 0.7421\n",
            "Epoch 215/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5230 - accuracy: 0.7557 - val_loss: 0.5131 - val_accuracy: 0.7421\n",
            "Epoch 216/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5228 - accuracy: 0.7557 - val_loss: 0.5128 - val_accuracy: 0.7421\n",
            "Epoch 217/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5225 - accuracy: 0.7557 - val_loss: 0.5125 - val_accuracy: 0.7421\n",
            "Epoch 218/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5223 - accuracy: 0.7557 - val_loss: 0.5122 - val_accuracy: 0.7421\n",
            "Epoch 219/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5220 - accuracy: 0.7557 - val_loss: 0.5119 - val_accuracy: 0.7421\n",
            "Epoch 220/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5218 - accuracy: 0.7557 - val_loss: 0.5116 - val_accuracy: 0.7421\n",
            "Epoch 221/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5215 - accuracy: 0.7557 - val_loss: 0.5113 - val_accuracy: 0.7421\n",
            "Epoch 222/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5213 - accuracy: 0.7557 - val_loss: 0.5110 - val_accuracy: 0.7421\n",
            "Epoch 223/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5210 - accuracy: 0.7575 - val_loss: 0.5107 - val_accuracy: 0.7421\n",
            "Epoch 224/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5208 - accuracy: 0.7575 - val_loss: 0.5104 - val_accuracy: 0.7421\n",
            "Epoch 225/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5205 - accuracy: 0.7575 - val_loss: 0.5101 - val_accuracy: 0.7474\n",
            "Epoch 226/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5203 - accuracy: 0.7557 - val_loss: 0.5098 - val_accuracy: 0.7474\n",
            "Epoch 227/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5201 - accuracy: 0.7557 - val_loss: 0.5095 - val_accuracy: 0.7526\n",
            "Epoch 228/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5198 - accuracy: 0.7557 - val_loss: 0.5092 - val_accuracy: 0.7526\n",
            "Epoch 229/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5196 - accuracy: 0.7557 - val_loss: 0.5089 - val_accuracy: 0.7526\n",
            "Epoch 230/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5193 - accuracy: 0.7557 - val_loss: 0.5086 - val_accuracy: 0.7526\n",
            "Epoch 231/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5191 - accuracy: 0.7557 - val_loss: 0.5084 - val_accuracy: 0.7526\n",
            "Epoch 232/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5189 - accuracy: 0.7575 - val_loss: 0.5081 - val_accuracy: 0.7526\n",
            "Epoch 233/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5187 - accuracy: 0.7575 - val_loss: 0.5078 - val_accuracy: 0.7526\n",
            "Epoch 234/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5184 - accuracy: 0.7557 - val_loss: 0.5075 - val_accuracy: 0.7474\n",
            "Epoch 235/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5182 - accuracy: 0.7540 - val_loss: 0.5073 - val_accuracy: 0.7474\n",
            "Epoch 236/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5180 - accuracy: 0.7540 - val_loss: 0.5070 - val_accuracy: 0.7474\n",
            "Epoch 237/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5177 - accuracy: 0.7540 - val_loss: 0.5067 - val_accuracy: 0.7474\n",
            "Epoch 238/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5175 - accuracy: 0.7540 - val_loss: 0.5065 - val_accuracy: 0.7526\n",
            "Epoch 239/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5173 - accuracy: 0.7557 - val_loss: 0.5062 - val_accuracy: 0.7526\n",
            "Epoch 240/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5171 - accuracy: 0.7540 - val_loss: 0.5059 - val_accuracy: 0.7526\n",
            "Epoch 241/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5169 - accuracy: 0.7557 - val_loss: 0.5057 - val_accuracy: 0.7526\n",
            "Epoch 242/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5167 - accuracy: 0.7557 - val_loss: 0.5054 - val_accuracy: 0.7526\n",
            "Epoch 243/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5165 - accuracy: 0.7557 - val_loss: 0.5052 - val_accuracy: 0.7526\n",
            "Epoch 244/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5163 - accuracy: 0.7557 - val_loss: 0.5049 - val_accuracy: 0.7526\n",
            "Epoch 245/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5160 - accuracy: 0.7557 - val_loss: 0.5047 - val_accuracy: 0.7526\n",
            "Epoch 246/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5158 - accuracy: 0.7557 - val_loss: 0.5044 - val_accuracy: 0.7526\n",
            "Epoch 247/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5157 - accuracy: 0.7557 - val_loss: 0.5042 - val_accuracy: 0.7526\n",
            "Epoch 248/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5155 - accuracy: 0.7557 - val_loss: 0.5039 - val_accuracy: 0.7526\n",
            "Epoch 249/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5152 - accuracy: 0.7557 - val_loss: 0.5037 - val_accuracy: 0.7526\n",
            "Epoch 250/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5151 - accuracy: 0.7575 - val_loss: 0.5034 - val_accuracy: 0.7526\n",
            "Epoch 251/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5148 - accuracy: 0.7557 - val_loss: 0.5032 - val_accuracy: 0.7526\n",
            "Epoch 252/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5146 - accuracy: 0.7557 - val_loss: 0.5030 - val_accuracy: 0.7579\n",
            "Epoch 253/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5145 - accuracy: 0.7575 - val_loss: 0.5027 - val_accuracy: 0.7579\n",
            "Epoch 254/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5143 - accuracy: 0.7592 - val_loss: 0.5025 - val_accuracy: 0.7579\n",
            "Epoch 255/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5141 - accuracy: 0.7575 - val_loss: 0.5023 - val_accuracy: 0.7579\n",
            "Epoch 256/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5139 - accuracy: 0.7592 - val_loss: 0.5020 - val_accuracy: 0.7579\n",
            "Epoch 257/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5137 - accuracy: 0.7592 - val_loss: 0.5018 - val_accuracy: 0.7579\n",
            "Epoch 258/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5135 - accuracy: 0.7592 - val_loss: 0.5016 - val_accuracy: 0.7579\n",
            "Epoch 259/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5133 - accuracy: 0.7610 - val_loss: 0.5014 - val_accuracy: 0.7579\n",
            "Epoch 260/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5132 - accuracy: 0.7627 - val_loss: 0.5012 - val_accuracy: 0.7579\n",
            "Epoch 261/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5130 - accuracy: 0.7610 - val_loss: 0.5009 - val_accuracy: 0.7632\n",
            "Epoch 262/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5128 - accuracy: 0.7610 - val_loss: 0.5007 - val_accuracy: 0.7632\n",
            "Epoch 263/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5126 - accuracy: 0.7627 - val_loss: 0.5005 - val_accuracy: 0.7684\n",
            "Epoch 264/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5124 - accuracy: 0.7627 - val_loss: 0.5003 - val_accuracy: 0.7684\n",
            "Epoch 265/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5122 - accuracy: 0.7645 - val_loss: 0.5000 - val_accuracy: 0.7684\n",
            "Epoch 266/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5121 - accuracy: 0.7645 - val_loss: 0.4998 - val_accuracy: 0.7684\n",
            "Epoch 267/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5119 - accuracy: 0.7645 - val_loss: 0.4996 - val_accuracy: 0.7684\n",
            "Epoch 268/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5117 - accuracy: 0.7645 - val_loss: 0.4994 - val_accuracy: 0.7684\n",
            "Epoch 269/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5115 - accuracy: 0.7645 - val_loss: 0.4992 - val_accuracy: 0.7684\n",
            "Epoch 270/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5114 - accuracy: 0.7645 - val_loss: 0.4990 - val_accuracy: 0.7684\n",
            "Epoch 271/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5112 - accuracy: 0.7645 - val_loss: 0.4988 - val_accuracy: 0.7684\n",
            "Epoch 272/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5110 - accuracy: 0.7645 - val_loss: 0.4985 - val_accuracy: 0.7684\n",
            "Epoch 273/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5109 - accuracy: 0.7645 - val_loss: 0.4983 - val_accuracy: 0.7684\n",
            "Epoch 274/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5107 - accuracy: 0.7627 - val_loss: 0.4981 - val_accuracy: 0.7684\n",
            "Epoch 275/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5105 - accuracy: 0.7627 - val_loss: 0.4979 - val_accuracy: 0.7684\n",
            "Epoch 276/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5103 - accuracy: 0.7627 - val_loss: 0.4977 - val_accuracy: 0.7684\n",
            "Epoch 277/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5102 - accuracy: 0.7627 - val_loss: 0.4975 - val_accuracy: 0.7684\n",
            "Epoch 278/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5100 - accuracy: 0.7627 - val_loss: 0.4974 - val_accuracy: 0.7684\n",
            "Epoch 279/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5099 - accuracy: 0.7627 - val_loss: 0.4972 - val_accuracy: 0.7684\n",
            "Epoch 280/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5097 - accuracy: 0.7627 - val_loss: 0.4970 - val_accuracy: 0.7684\n",
            "Epoch 281/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5095 - accuracy: 0.7627 - val_loss: 0.4968 - val_accuracy: 0.7737\n",
            "Epoch 282/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5094 - accuracy: 0.7627 - val_loss: 0.4966 - val_accuracy: 0.7737\n",
            "Epoch 283/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5092 - accuracy: 0.7627 - val_loss: 0.4964 - val_accuracy: 0.7737\n",
            "Epoch 284/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5091 - accuracy: 0.7627 - val_loss: 0.4962 - val_accuracy: 0.7737\n",
            "Epoch 285/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5089 - accuracy: 0.7627 - val_loss: 0.4960 - val_accuracy: 0.7737\n",
            "Epoch 286/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5088 - accuracy: 0.7627 - val_loss: 0.4958 - val_accuracy: 0.7737\n",
            "Epoch 287/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5086 - accuracy: 0.7645 - val_loss: 0.4956 - val_accuracy: 0.7737\n",
            "Epoch 288/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5084 - accuracy: 0.7645 - val_loss: 0.4954 - val_accuracy: 0.7789\n",
            "Epoch 289/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5083 - accuracy: 0.7663 - val_loss: 0.4952 - val_accuracy: 0.7789\n",
            "Epoch 290/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5082 - accuracy: 0.7645 - val_loss: 0.4950 - val_accuracy: 0.7789\n",
            "Epoch 291/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5080 - accuracy: 0.7663 - val_loss: 0.4948 - val_accuracy: 0.7789\n",
            "Epoch 292/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5078 - accuracy: 0.7663 - val_loss: 0.4947 - val_accuracy: 0.7789\n",
            "Epoch 293/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5077 - accuracy: 0.7663 - val_loss: 0.4945 - val_accuracy: 0.7789\n",
            "Epoch 294/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5076 - accuracy: 0.7663 - val_loss: 0.4943 - val_accuracy: 0.7789\n",
            "Epoch 295/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5074 - accuracy: 0.7663 - val_loss: 0.4941 - val_accuracy: 0.7842\n",
            "Epoch 296/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5073 - accuracy: 0.7663 - val_loss: 0.4940 - val_accuracy: 0.7842\n",
            "Epoch 297/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5071 - accuracy: 0.7663 - val_loss: 0.4938 - val_accuracy: 0.7842\n",
            "Epoch 298/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5070 - accuracy: 0.7663 - val_loss: 0.4936 - val_accuracy: 0.7842\n",
            "Epoch 299/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5068 - accuracy: 0.7663 - val_loss: 0.4934 - val_accuracy: 0.7842\n",
            "Epoch 300/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5067 - accuracy: 0.7663 - val_loss: 0.4933 - val_accuracy: 0.7842\n",
            "Epoch 301/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5065 - accuracy: 0.7663 - val_loss: 0.4931 - val_accuracy: 0.7842\n",
            "Epoch 302/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5064 - accuracy: 0.7663 - val_loss: 0.4929 - val_accuracy: 0.7842\n",
            "Epoch 303/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5063 - accuracy: 0.7663 - val_loss: 0.4927 - val_accuracy: 0.7842\n",
            "Epoch 304/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5062 - accuracy: 0.7663 - val_loss: 0.4926 - val_accuracy: 0.7842\n",
            "Epoch 305/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5060 - accuracy: 0.7645 - val_loss: 0.4924 - val_accuracy: 0.7842\n",
            "Epoch 306/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5059 - accuracy: 0.7645 - val_loss: 0.4922 - val_accuracy: 0.7842\n",
            "Epoch 307/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5057 - accuracy: 0.7663 - val_loss: 0.4921 - val_accuracy: 0.7842\n",
            "Epoch 308/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5056 - accuracy: 0.7645 - val_loss: 0.4919 - val_accuracy: 0.7842\n",
            "Epoch 309/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5055 - accuracy: 0.7645 - val_loss: 0.4917 - val_accuracy: 0.7842\n",
            "Epoch 310/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5053 - accuracy: 0.7645 - val_loss: 0.4916 - val_accuracy: 0.7842\n",
            "Epoch 311/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5052 - accuracy: 0.7645 - val_loss: 0.4914 - val_accuracy: 0.7842\n",
            "Epoch 312/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5051 - accuracy: 0.7645 - val_loss: 0.4913 - val_accuracy: 0.7842\n",
            "Epoch 313/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5050 - accuracy: 0.7645 - val_loss: 0.4911 - val_accuracy: 0.7842\n",
            "Epoch 314/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5049 - accuracy: 0.7645 - val_loss: 0.4909 - val_accuracy: 0.7842\n",
            "Epoch 315/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5047 - accuracy: 0.7645 - val_loss: 0.4908 - val_accuracy: 0.7842\n",
            "Epoch 316/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5046 - accuracy: 0.7645 - val_loss: 0.4906 - val_accuracy: 0.7842\n",
            "Epoch 317/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5044 - accuracy: 0.7645 - val_loss: 0.4905 - val_accuracy: 0.7842\n",
            "Epoch 318/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5043 - accuracy: 0.7645 - val_loss: 0.4903 - val_accuracy: 0.7895\n",
            "Epoch 319/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5042 - accuracy: 0.7627 - val_loss: 0.4902 - val_accuracy: 0.7895\n",
            "Epoch 320/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5041 - accuracy: 0.7627 - val_loss: 0.4900 - val_accuracy: 0.7895\n",
            "Epoch 321/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5040 - accuracy: 0.7627 - val_loss: 0.4898 - val_accuracy: 0.7895\n",
            "Epoch 322/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5038 - accuracy: 0.7627 - val_loss: 0.4897 - val_accuracy: 0.7895\n",
            "Epoch 323/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5037 - accuracy: 0.7627 - val_loss: 0.4895 - val_accuracy: 0.7895\n",
            "Epoch 324/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5036 - accuracy: 0.7645 - val_loss: 0.4894 - val_accuracy: 0.7895\n",
            "Epoch 325/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5035 - accuracy: 0.7645 - val_loss: 0.4892 - val_accuracy: 0.7895\n",
            "Epoch 326/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5034 - accuracy: 0.7645 - val_loss: 0.4891 - val_accuracy: 0.7895\n",
            "Epoch 327/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5033 - accuracy: 0.7645 - val_loss: 0.4890 - val_accuracy: 0.7895\n",
            "Epoch 328/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5032 - accuracy: 0.7645 - val_loss: 0.4888 - val_accuracy: 0.7895\n",
            "Epoch 329/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5030 - accuracy: 0.7645 - val_loss: 0.4887 - val_accuracy: 0.7895\n",
            "Epoch 330/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5029 - accuracy: 0.7645 - val_loss: 0.4885 - val_accuracy: 0.7895\n",
            "Epoch 331/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5028 - accuracy: 0.7645 - val_loss: 0.4884 - val_accuracy: 0.7895\n",
            "Epoch 332/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5027 - accuracy: 0.7645 - val_loss: 0.4882 - val_accuracy: 0.7895\n",
            "Epoch 333/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5026 - accuracy: 0.7645 - val_loss: 0.4881 - val_accuracy: 0.7895\n",
            "Epoch 334/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5025 - accuracy: 0.7645 - val_loss: 0.4880 - val_accuracy: 0.7895\n",
            "Epoch 335/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5024 - accuracy: 0.7645 - val_loss: 0.4878 - val_accuracy: 0.7895\n",
            "Epoch 336/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5022 - accuracy: 0.7645 - val_loss: 0.4877 - val_accuracy: 0.7895\n",
            "Epoch 337/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5021 - accuracy: 0.7663 - val_loss: 0.4875 - val_accuracy: 0.7895\n",
            "Epoch 338/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5020 - accuracy: 0.7663 - val_loss: 0.4874 - val_accuracy: 0.7895\n",
            "Epoch 339/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5019 - accuracy: 0.7645 - val_loss: 0.4873 - val_accuracy: 0.7895\n",
            "Epoch 340/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5018 - accuracy: 0.7663 - val_loss: 0.4871 - val_accuracy: 0.7895\n",
            "Epoch 341/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5017 - accuracy: 0.7663 - val_loss: 0.4870 - val_accuracy: 0.7895\n",
            "Epoch 342/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5016 - accuracy: 0.7663 - val_loss: 0.4869 - val_accuracy: 0.7895\n",
            "Epoch 343/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5015 - accuracy: 0.7663 - val_loss: 0.4867 - val_accuracy: 0.7895\n",
            "Epoch 344/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5014 - accuracy: 0.7663 - val_loss: 0.4866 - val_accuracy: 0.7895\n",
            "Epoch 345/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.5013 - accuracy: 0.7663 - val_loss: 0.4865 - val_accuracy: 0.7895\n",
            "Epoch 346/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5012 - accuracy: 0.7663 - val_loss: 0.4863 - val_accuracy: 0.7895\n",
            "Epoch 347/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7663 - val_loss: 0.4862 - val_accuracy: 0.7895\n",
            "Epoch 348/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5010 - accuracy: 0.7663 - val_loss: 0.4861 - val_accuracy: 0.7895\n",
            "Epoch 349/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5009 - accuracy: 0.7663 - val_loss: 0.4859 - val_accuracy: 0.7842\n",
            "Epoch 350/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5008 - accuracy: 0.7663 - val_loss: 0.4858 - val_accuracy: 0.7842\n",
            "Epoch 351/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5007 - accuracy: 0.7680 - val_loss: 0.4857 - val_accuracy: 0.7842\n",
            "Epoch 352/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5006 - accuracy: 0.7663 - val_loss: 0.4856 - val_accuracy: 0.7842\n",
            "Epoch 353/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5005 - accuracy: 0.7663 - val_loss: 0.4854 - val_accuracy: 0.7842\n",
            "Epoch 354/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5004 - accuracy: 0.7680 - val_loss: 0.4853 - val_accuracy: 0.7842\n",
            "Epoch 355/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5003 - accuracy: 0.7680 - val_loss: 0.4852 - val_accuracy: 0.7895\n",
            "Epoch 356/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5002 - accuracy: 0.7663 - val_loss: 0.4850 - val_accuracy: 0.7895\n",
            "Epoch 357/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5001 - accuracy: 0.7680 - val_loss: 0.4849 - val_accuracy: 0.7895\n",
            "Epoch 358/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5000 - accuracy: 0.7663 - val_loss: 0.4848 - val_accuracy: 0.7895\n",
            "Epoch 359/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4999 - accuracy: 0.7663 - val_loss: 0.4847 - val_accuracy: 0.7895\n",
            "Epoch 360/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4998 - accuracy: 0.7680 - val_loss: 0.4846 - val_accuracy: 0.7895\n",
            "Epoch 361/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4998 - accuracy: 0.7663 - val_loss: 0.4844 - val_accuracy: 0.7895\n",
            "Epoch 362/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4996 - accuracy: 0.7663 - val_loss: 0.4843 - val_accuracy: 0.7895\n",
            "Epoch 363/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4995 - accuracy: 0.7645 - val_loss: 0.4842 - val_accuracy: 0.7895\n",
            "Epoch 364/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4994 - accuracy: 0.7645 - val_loss: 0.4841 - val_accuracy: 0.7895\n",
            "Epoch 365/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4993 - accuracy: 0.7645 - val_loss: 0.4840 - val_accuracy: 0.7895\n",
            "Epoch 366/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4992 - accuracy: 0.7645 - val_loss: 0.4839 - val_accuracy: 0.7895\n",
            "Epoch 367/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4992 - accuracy: 0.7645 - val_loss: 0.4837 - val_accuracy: 0.7895\n",
            "Epoch 368/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4991 - accuracy: 0.7645 - val_loss: 0.4836 - val_accuracy: 0.7895\n",
            "Epoch 369/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4990 - accuracy: 0.7645 - val_loss: 0.4835 - val_accuracy: 0.7895\n",
            "Epoch 370/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4989 - accuracy: 0.7645 - val_loss: 0.4834 - val_accuracy: 0.7895\n",
            "Epoch 371/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4988 - accuracy: 0.7645 - val_loss: 0.4833 - val_accuracy: 0.7895\n",
            "Epoch 372/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4987 - accuracy: 0.7645 - val_loss: 0.4832 - val_accuracy: 0.7895\n",
            "Epoch 373/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4986 - accuracy: 0.7645 - val_loss: 0.4831 - val_accuracy: 0.7895\n",
            "Epoch 374/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4985 - accuracy: 0.7627 - val_loss: 0.4830 - val_accuracy: 0.7895\n",
            "Epoch 375/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4984 - accuracy: 0.7645 - val_loss: 0.4828 - val_accuracy: 0.7895\n",
            "Epoch 376/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4984 - accuracy: 0.7645 - val_loss: 0.4827 - val_accuracy: 0.7895\n",
            "Epoch 377/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4983 - accuracy: 0.7627 - val_loss: 0.4826 - val_accuracy: 0.7895\n",
            "Epoch 378/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4982 - accuracy: 0.7645 - val_loss: 0.4825 - val_accuracy: 0.7895\n",
            "Epoch 379/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4981 - accuracy: 0.7627 - val_loss: 0.4824 - val_accuracy: 0.7842\n",
            "Epoch 380/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4980 - accuracy: 0.7627 - val_loss: 0.4823 - val_accuracy: 0.7842\n",
            "Epoch 381/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4979 - accuracy: 0.7627 - val_loss: 0.4822 - val_accuracy: 0.7842\n",
            "Epoch 382/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4979 - accuracy: 0.7645 - val_loss: 0.4821 - val_accuracy: 0.7842\n",
            "Epoch 383/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4978 - accuracy: 0.7645 - val_loss: 0.4820 - val_accuracy: 0.7842\n",
            "Epoch 384/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4977 - accuracy: 0.7645 - val_loss: 0.4819 - val_accuracy: 0.7842\n",
            "Epoch 385/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4976 - accuracy: 0.7645 - val_loss: 0.4818 - val_accuracy: 0.7842\n",
            "Epoch 386/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4975 - accuracy: 0.7645 - val_loss: 0.4817 - val_accuracy: 0.7842\n",
            "Epoch 387/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4974 - accuracy: 0.7645 - val_loss: 0.4815 - val_accuracy: 0.7842\n",
            "Epoch 388/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4973 - accuracy: 0.7645 - val_loss: 0.4814 - val_accuracy: 0.7789\n",
            "Epoch 389/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4973 - accuracy: 0.7645 - val_loss: 0.4813 - val_accuracy: 0.7789\n",
            "Epoch 390/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4972 - accuracy: 0.7645 - val_loss: 0.4812 - val_accuracy: 0.7789\n",
            "Epoch 391/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4971 - accuracy: 0.7645 - val_loss: 0.4811 - val_accuracy: 0.7789\n",
            "Epoch 392/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4971 - accuracy: 0.7663 - val_loss: 0.4810 - val_accuracy: 0.7789\n",
            "Epoch 393/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4970 - accuracy: 0.7663 - val_loss: 0.4809 - val_accuracy: 0.7789\n",
            "Epoch 394/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4969 - accuracy: 0.7663 - val_loss: 0.4808 - val_accuracy: 0.7789\n",
            "Epoch 395/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4968 - accuracy: 0.7645 - val_loss: 0.4807 - val_accuracy: 0.7789\n",
            "Epoch 396/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4967 - accuracy: 0.7645 - val_loss: 0.4806 - val_accuracy: 0.7789\n",
            "Epoch 397/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4966 - accuracy: 0.7645 - val_loss: 0.4805 - val_accuracy: 0.7789\n",
            "Epoch 398/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4966 - accuracy: 0.7663 - val_loss: 0.4804 - val_accuracy: 0.7789\n",
            "Epoch 399/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4965 - accuracy: 0.7680 - val_loss: 0.4803 - val_accuracy: 0.7789\n",
            "Epoch 400/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4964 - accuracy: 0.7680 - val_loss: 0.4802 - val_accuracy: 0.7789\n",
            "Epoch 401/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4963 - accuracy: 0.7680 - val_loss: 0.4801 - val_accuracy: 0.7789\n",
            "Epoch 402/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4963 - accuracy: 0.7680 - val_loss: 0.4800 - val_accuracy: 0.7789\n",
            "Epoch 403/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4962 - accuracy: 0.7680 - val_loss: 0.4799 - val_accuracy: 0.7789\n",
            "Epoch 404/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4962 - accuracy: 0.7680 - val_loss: 0.4799 - val_accuracy: 0.7789\n",
            "Epoch 405/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4961 - accuracy: 0.7680 - val_loss: 0.4798 - val_accuracy: 0.7789\n",
            "Epoch 406/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4960 - accuracy: 0.7680 - val_loss: 0.4797 - val_accuracy: 0.7789\n",
            "Epoch 407/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4959 - accuracy: 0.7680 - val_loss: 0.4796 - val_accuracy: 0.7789\n",
            "Epoch 408/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4958 - accuracy: 0.7680 - val_loss: 0.4795 - val_accuracy: 0.7789\n",
            "Epoch 409/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4958 - accuracy: 0.7680 - val_loss: 0.4794 - val_accuracy: 0.7789\n",
            "Epoch 410/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4957 - accuracy: 0.7680 - val_loss: 0.4793 - val_accuracy: 0.7789\n",
            "Epoch 411/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4956 - accuracy: 0.7680 - val_loss: 0.4792 - val_accuracy: 0.7789\n",
            "Epoch 412/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4955 - accuracy: 0.7663 - val_loss: 0.4791 - val_accuracy: 0.7789\n",
            "Epoch 413/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4955 - accuracy: 0.7663 - val_loss: 0.4790 - val_accuracy: 0.7789\n",
            "Epoch 414/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4954 - accuracy: 0.7663 - val_loss: 0.4789 - val_accuracy: 0.7737\n",
            "Epoch 415/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4953 - accuracy: 0.7663 - val_loss: 0.4789 - val_accuracy: 0.7737\n",
            "Epoch 416/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4953 - accuracy: 0.7663 - val_loss: 0.4788 - val_accuracy: 0.7737\n",
            "Epoch 417/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4952 - accuracy: 0.7663 - val_loss: 0.4787 - val_accuracy: 0.7737\n",
            "Epoch 418/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4952 - accuracy: 0.7663 - val_loss: 0.4786 - val_accuracy: 0.7737\n",
            "Epoch 419/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4951 - accuracy: 0.7663 - val_loss: 0.4785 - val_accuracy: 0.7737\n",
            "Epoch 420/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4950 - accuracy: 0.7663 - val_loss: 0.4784 - val_accuracy: 0.7737\n",
            "Epoch 421/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4950 - accuracy: 0.7663 - val_loss: 0.4783 - val_accuracy: 0.7737\n",
            "Epoch 422/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4949 - accuracy: 0.7663 - val_loss: 0.4782 - val_accuracy: 0.7737\n",
            "Epoch 423/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4948 - accuracy: 0.7663 - val_loss: 0.4782 - val_accuracy: 0.7789\n",
            "Epoch 424/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4947 - accuracy: 0.7663 - val_loss: 0.4781 - val_accuracy: 0.7789\n",
            "Epoch 425/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4947 - accuracy: 0.7663 - val_loss: 0.4780 - val_accuracy: 0.7789\n",
            "Epoch 426/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4946 - accuracy: 0.7663 - val_loss: 0.4779 - val_accuracy: 0.7789\n",
            "Epoch 427/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4946 - accuracy: 0.7663 - val_loss: 0.4778 - val_accuracy: 0.7789\n",
            "Epoch 428/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4945 - accuracy: 0.7663 - val_loss: 0.4777 - val_accuracy: 0.7789\n",
            "Epoch 429/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4944 - accuracy: 0.7663 - val_loss: 0.4776 - val_accuracy: 0.7789\n",
            "Epoch 430/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4944 - accuracy: 0.7663 - val_loss: 0.4776 - val_accuracy: 0.7789\n",
            "Epoch 431/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4943 - accuracy: 0.7663 - val_loss: 0.4775 - val_accuracy: 0.7789\n",
            "Epoch 432/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4942 - accuracy: 0.7645 - val_loss: 0.4774 - val_accuracy: 0.7789\n",
            "Epoch 433/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4942 - accuracy: 0.7663 - val_loss: 0.4773 - val_accuracy: 0.7789\n",
            "Epoch 434/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4941 - accuracy: 0.7680 - val_loss: 0.4772 - val_accuracy: 0.7789\n",
            "Epoch 435/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4941 - accuracy: 0.7663 - val_loss: 0.4771 - val_accuracy: 0.7789\n",
            "Epoch 436/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4940 - accuracy: 0.7645 - val_loss: 0.4771 - val_accuracy: 0.7789\n",
            "Epoch 437/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4939 - accuracy: 0.7627 - val_loss: 0.4770 - val_accuracy: 0.7789\n",
            "Epoch 438/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4939 - accuracy: 0.7627 - val_loss: 0.4769 - val_accuracy: 0.7789\n",
            "Epoch 439/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4938 - accuracy: 0.7627 - val_loss: 0.4768 - val_accuracy: 0.7789\n",
            "Epoch 440/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4938 - accuracy: 0.7627 - val_loss: 0.4767 - val_accuracy: 0.7789\n",
            "Epoch 441/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4937 - accuracy: 0.7627 - val_loss: 0.4767 - val_accuracy: 0.7789\n",
            "Epoch 442/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4936 - accuracy: 0.7610 - val_loss: 0.4766 - val_accuracy: 0.7789\n",
            "Epoch 443/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4936 - accuracy: 0.7610 - val_loss: 0.4765 - val_accuracy: 0.7789\n",
            "Epoch 444/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4935 - accuracy: 0.7610 - val_loss: 0.4764 - val_accuracy: 0.7789\n",
            "Epoch 445/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4935 - accuracy: 0.7610 - val_loss: 0.4764 - val_accuracy: 0.7789\n",
            "Epoch 446/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4934 - accuracy: 0.7610 - val_loss: 0.4763 - val_accuracy: 0.7789\n",
            "Epoch 447/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4933 - accuracy: 0.7610 - val_loss: 0.4762 - val_accuracy: 0.7789\n",
            "Epoch 448/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4933 - accuracy: 0.7610 - val_loss: 0.4761 - val_accuracy: 0.7789\n",
            "Epoch 449/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4932 - accuracy: 0.7610 - val_loss: 0.4761 - val_accuracy: 0.7789\n",
            "Epoch 450/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4932 - accuracy: 0.7610 - val_loss: 0.4760 - val_accuracy: 0.7789\n",
            "Epoch 451/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4931 - accuracy: 0.7610 - val_loss: 0.4759 - val_accuracy: 0.7789\n",
            "Epoch 452/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4931 - accuracy: 0.7627 - val_loss: 0.4758 - val_accuracy: 0.7789\n",
            "Epoch 453/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4930 - accuracy: 0.7592 - val_loss: 0.4758 - val_accuracy: 0.7789\n",
            "Epoch 454/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4929 - accuracy: 0.7610 - val_loss: 0.4757 - val_accuracy: 0.7789\n",
            "Epoch 455/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4929 - accuracy: 0.7610 - val_loss: 0.4756 - val_accuracy: 0.7789\n",
            "Epoch 456/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4928 - accuracy: 0.7610 - val_loss: 0.4755 - val_accuracy: 0.7789\n",
            "Epoch 457/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4928 - accuracy: 0.7627 - val_loss: 0.4755 - val_accuracy: 0.7789\n",
            "Epoch 458/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4927 - accuracy: 0.7627 - val_loss: 0.4754 - val_accuracy: 0.7789\n",
            "Epoch 459/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4927 - accuracy: 0.7627 - val_loss: 0.4753 - val_accuracy: 0.7789\n",
            "Epoch 460/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4926 - accuracy: 0.7627 - val_loss: 0.4753 - val_accuracy: 0.7789\n",
            "Epoch 461/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4926 - accuracy: 0.7627 - val_loss: 0.4752 - val_accuracy: 0.7789\n",
            "Epoch 462/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4925 - accuracy: 0.7627 - val_loss: 0.4751 - val_accuracy: 0.7789\n",
            "Epoch 463/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4925 - accuracy: 0.7627 - val_loss: 0.4750 - val_accuracy: 0.7789\n",
            "Epoch 464/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4924 - accuracy: 0.7627 - val_loss: 0.4750 - val_accuracy: 0.7789\n",
            "Epoch 465/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4924 - accuracy: 0.7627 - val_loss: 0.4749 - val_accuracy: 0.7789\n",
            "Epoch 466/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4923 - accuracy: 0.7627 - val_loss: 0.4749 - val_accuracy: 0.7789\n",
            "Epoch 467/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4923 - accuracy: 0.7627 - val_loss: 0.4748 - val_accuracy: 0.7789\n",
            "Epoch 468/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4922 - accuracy: 0.7627 - val_loss: 0.4747 - val_accuracy: 0.7789\n",
            "Epoch 469/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4921 - accuracy: 0.7627 - val_loss: 0.4746 - val_accuracy: 0.7789\n",
            "Epoch 470/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4921 - accuracy: 0.7627 - val_loss: 0.4746 - val_accuracy: 0.7789\n",
            "Epoch 471/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4921 - accuracy: 0.7627 - val_loss: 0.4745 - val_accuracy: 0.7789\n",
            "Epoch 472/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4920 - accuracy: 0.7627 - val_loss: 0.4744 - val_accuracy: 0.7789\n",
            "Epoch 473/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4920 - accuracy: 0.7627 - val_loss: 0.4744 - val_accuracy: 0.7789\n",
            "Epoch 474/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4919 - accuracy: 0.7627 - val_loss: 0.4743 - val_accuracy: 0.7789\n",
            "Epoch 475/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4919 - accuracy: 0.7627 - val_loss: 0.4742 - val_accuracy: 0.7789\n",
            "Epoch 476/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4918 - accuracy: 0.7627 - val_loss: 0.4742 - val_accuracy: 0.7789\n",
            "Epoch 477/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4918 - accuracy: 0.7627 - val_loss: 0.4741 - val_accuracy: 0.7789\n",
            "Epoch 478/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4917 - accuracy: 0.7627 - val_loss: 0.4740 - val_accuracy: 0.7789\n",
            "Epoch 479/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4916 - accuracy: 0.7627 - val_loss: 0.4740 - val_accuracy: 0.7789\n",
            "Epoch 480/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4916 - accuracy: 0.7627 - val_loss: 0.4739 - val_accuracy: 0.7789\n",
            "Epoch 481/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4915 - accuracy: 0.7627 - val_loss: 0.4738 - val_accuracy: 0.7789\n",
            "Epoch 482/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4915 - accuracy: 0.7627 - val_loss: 0.4738 - val_accuracy: 0.7789\n",
            "Epoch 483/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4915 - accuracy: 0.7627 - val_loss: 0.4737 - val_accuracy: 0.7789\n",
            "Epoch 484/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4914 - accuracy: 0.7627 - val_loss: 0.4737 - val_accuracy: 0.7789\n",
            "Epoch 485/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4914 - accuracy: 0.7627 - val_loss: 0.4736 - val_accuracy: 0.7789\n",
            "Epoch 486/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4913 - accuracy: 0.7627 - val_loss: 0.4735 - val_accuracy: 0.7789\n",
            "Epoch 487/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4913 - accuracy: 0.7627 - val_loss: 0.4735 - val_accuracy: 0.7789\n",
            "Epoch 488/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4912 - accuracy: 0.7627 - val_loss: 0.4734 - val_accuracy: 0.7789\n",
            "Epoch 489/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4912 - accuracy: 0.7627 - val_loss: 0.4733 - val_accuracy: 0.7789\n",
            "Epoch 490/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4911 - accuracy: 0.7610 - val_loss: 0.4733 - val_accuracy: 0.7789\n",
            "Epoch 491/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4911 - accuracy: 0.7627 - val_loss: 0.4732 - val_accuracy: 0.7789\n",
            "Epoch 492/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4910 - accuracy: 0.7627 - val_loss: 0.4732 - val_accuracy: 0.7789\n",
            "Epoch 493/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4910 - accuracy: 0.7627 - val_loss: 0.4731 - val_accuracy: 0.7789\n",
            "Epoch 494/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4909 - accuracy: 0.7610 - val_loss: 0.4730 - val_accuracy: 0.7789\n",
            "Epoch 495/500\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4909 - accuracy: 0.7627 - val_loss: 0.4730 - val_accuracy: 0.7789\n",
            "Epoch 496/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4908 - accuracy: 0.7610 - val_loss: 0.4729 - val_accuracy: 0.7789\n",
            "Epoch 497/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4908 - accuracy: 0.7627 - val_loss: 0.4728 - val_accuracy: 0.7789\n",
            "Epoch 498/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4908 - accuracy: 0.7627 - val_loss: 0.4728 - val_accuracy: 0.7789\n",
            "Epoch 499/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4907 - accuracy: 0.7610 - val_loss: 0.4727 - val_accuracy: 0.7789\n",
            "Epoch 500/500\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4907 - accuracy: 0.7627 - val_loss: 0.4727 - val_accuracy: 0.7789\n",
            "\n",
            "Elapsed Time =>  0:00:38.532567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvDlgjzp4sf8",
        "colab_type": "code",
        "outputId": "0a779eb3-44ea-4e91-99ce-476ead261262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(x_data, t_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.7668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.48610225319862366, 0.7667984366416931]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJRl77mYINzR",
        "colab_type": "code",
        "outputId": "64d435fe-aec0-424a-a3b1-e3a58132aeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# hist 타입 확인\n",
        "\n",
        "print(type(hist))\n",
        "\n",
        "print(type(hist.history))\n",
        "\n",
        "# loss & accuracy of train data => hist.history['loss'], hist.history['accuracy']\n",
        "# loss & accuracy of validation data => hist.history['val_loss'], hist.history['val_accuracy']\n",
        "\n",
        "print(hist.history.keys())  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.keras.callbacks.History'>\n",
            "<class 'dict'>\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLvLWpcUCPwM",
        "colab_type": "code",
        "outputId": "0ae1a5f2-3a95-4a19-f378-6ff46d48f44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# loss overfitting 확인\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Loss Trend')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "\n",
        "plt.plot(hist.history['loss'], label='train loss')\n",
        "plt.plot(hist.history['val_loss'], label='validation loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikK2aQ9sEMqj",
        "colab_type": "code",
        "outputId": "5e56d18b-73eb-4b88-8c69-16146518f2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# accuracy overfitting 확인\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Accuracy Trend')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "\n",
        "plt.plot(hist.history['accuracy'], label='train accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], label='validation accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU-BDTdKEfWW",
        "colab_type": "code",
        "outputId": "f2f08232-122a-4654-c45f-8a7bdc023b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# index_label_prediction_list 만들기 위한 1st step\n",
        "\n",
        "real_prediction_val = model.predict(x_data)\n",
        "\n",
        "print(type(real_prediction_val), len(real_prediction_val))\n",
        "\n",
        "print(real_prediction_val[:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> 759\n",
            "[[0.449366  ]\n",
            " [0.910043  ]\n",
            " [0.25367475]\n",
            " [0.9340566 ]\n",
            " [0.17188057]\n",
            " [0.74765295]\n",
            " [0.9189131 ]\n",
            " [0.57958335]\n",
            " [0.31601265]\n",
            " [0.57250726]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woENLDOMFqtd",
        "colab_type": "code",
        "outputId": "1be02e08-9817-468c-ba8f-c1df7bdb68d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "# index_label_prediction_list 만들기 위한 2nd step\n",
        "\n",
        "logical_prediction_val = (real_prediction_val > 0.5)\n",
        "\n",
        "print(type(logical_prediction_val), len(logical_prediction_val))\n",
        "\n",
        "print(logical_prediction_val[:10])\n",
        "\n",
        "# 결과 가독성 높이기 위해 flatten() 이용해서 1차원 변환\n",
        "# True -> 1.0, False -> 0.0  astype('float32') 이용해서 타입캐스팅\n",
        "\n",
        "logical_prediction_val = logical_prediction_val.flatten().astype('float32')\n",
        "\n",
        "print(type(logical_prediction_val), len(logical_prediction_val))\n",
        "\n",
        "print(logical_prediction_val[:10])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> 759\n",
            "[[False]\n",
            " [ True]\n",
            " [False]\n",
            " [ True]\n",
            " [False]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [False]\n",
            " [ True]]\n",
            "<class 'numpy.ndarray'> 759\n",
            "[0. 1. 0. 1. 0. 1. 1. 1. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKy_MIppFAff",
        "colab_type": "code",
        "outputId": "0d1210d6-3923-4134-f797-f8632b557897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# index_label_prediction_list 만들기 위한 3rd step\n",
        "\n",
        "label = t_data.flatten()\n",
        "\n",
        "comp_result = np.equal(label, logical_prediction_val)\n",
        "\n",
        "print(type(comp_result), len(comp_result))\n",
        "\n",
        "print('comp result = ', comp_result[:10])\n",
        "print('label       = ', label[:10])\n",
        "print('prediction  = ', logical_prediction_val[:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> 759\n",
            "comp result =  [ True  True  True  True  True  True False  True  True False]\n",
            "label       =  [0. 1. 0. 1. 0. 1. 0. 1. 0. 0.]\n",
            "prediction  =  [0. 1. 0. 1. 0. 1. 1. 1. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj5tuBUoGq0k",
        "colab_type": "code",
        "outputId": "beb73f01-02db-4f78-95b8-1cdeb94e1f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 오답에 대해 list comprehension 이용하여 index_label_prediction_list 구현\n",
        "\n",
        "index_label_prediction_list = [ [idx, label[idx], logical_prediction_val[idx] ]  for idx, value in enumerate(comp_result)  if value == False ]\n",
        "\n",
        "print(len(index_label_prediction_list))\n",
        "\n",
        "print('Accuracy => ', 1- (len(index_label_prediction_list))/(len(x_data)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "177\n",
            "Accuracy =>  0.766798418972332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGXXdOPVHiRk",
        "colab_type": "code",
        "outputId": "0d38325a-e458-4735-b2f3-d642609cfece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(index_label_prediction_list)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6, 0.0, 1.0], [9, 0.0, 1.0], [12, 1.0, 0.0], [14, 0.0, 1.0], [16, 0.0, 1.0], [18, 0.0, 1.0], [22, 0.0, 1.0], [23, 0.0, 1.0], [34, 1.0, 0.0], [35, 0.0, 1.0], [36, 0.0, 1.0], [37, 0.0, 1.0], [38, 1.0, 0.0], [39, 1.0, 0.0], [42, 1.0, 0.0], [46, 0.0, 1.0], [52, 1.0, 0.0], [55, 1.0, 0.0], [56, 1.0, 0.0], [59, 0.0, 1.0], [62, 0.0, 1.0], [64, 0.0, 1.0], [68, 0.0, 1.0], [91, 0.0, 1.0], [97, 0.0, 1.0], [107, 0.0, 1.0], [108, 0.0, 1.0], [113, 0.0, 1.0], [114, 0.0, 1.0], [122, 0.0, 1.0], [123, 0.0, 1.0], [126, 0.0, 1.0], [127, 0.0, 1.0], [128, 0.0, 1.0], [136, 1.0, 0.0], [141, 0.0, 1.0], [151, 1.0, 0.0], [163, 0.0, 1.0], [168, 0.0, 1.0], [169, 0.0, 1.0], [176, 1.0, 0.0], [185, 0.0, 1.0], [186, 0.0, 1.0], [187, 0.0, 1.0], [195, 0.0, 1.0], [196, 0.0, 1.0], [197, 0.0, 1.0], [209, 1.0, 0.0], [210, 1.0, 0.0], [212, 0.0, 1.0], [214, 0.0, 1.0], [216, 0.0, 1.0], [217, 0.0, 1.0], [226, 1.0, 0.0], [239, 0.0, 1.0], [240, 0.0, 1.0], [244, 1.0, 0.0], [251, 0.0, 1.0], [252, 0.0, 1.0], [255, 1.0, 0.0], [256, 1.0, 0.0], [257, 0.0, 1.0], [260, 0.0, 1.0], [263, 1.0, 0.0], [265, 0.0, 1.0], [272, 0.0, 1.0], [280, 0.0, 1.0], [281, 1.0, 0.0], [282, 0.0, 1.0], [286, 0.0, 1.0], [288, 0.0, 1.0], [289, 1.0, 0.0], [290, 1.0, 0.0], [291, 0.0, 1.0], [293, 0.0, 1.0], [296, 0.0, 1.0], [298, 0.0, 1.0], [301, 0.0, 1.0], [304, 0.0, 1.0], [307, 0.0, 1.0], [309, 0.0, 1.0], [316, 0.0, 1.0], [317, 0.0, 1.0], [321, 0.0, 1.0], [322, 1.0, 0.0], [323, 0.0, 1.0], [330, 1.0, 0.0], [331, 1.0, 0.0], [332, 0.0, 1.0], [344, 0.0, 1.0], [351, 0.0, 1.0], [356, 1.0, 0.0], [361, 0.0, 1.0], [364, 0.0, 1.0], [366, 1.0, 0.0], [381, 0.0, 1.0], [382, 0.0, 1.0], [383, 0.0, 1.0], [392, 0.0, 1.0], [395, 0.0, 1.0], [396, 0.0, 1.0], [400, 0.0, 1.0], [406, 1.0, 0.0], [408, 0.0, 1.0], [413, 0.0, 1.0], [422, 1.0, 0.0], [423, 0.0, 1.0], [430, 1.0, 0.0], [431, 1.0, 0.0], [437, 0.0, 1.0], [438, 0.0, 1.0], [442, 0.0, 1.0], [445, 0.0, 1.0], [461, 0.0, 1.0], [462, 1.0, 0.0], [469, 0.0, 1.0], [473, 0.0, 1.0], [480, 1.0, 0.0], [482, 1.0, 0.0], [486, 0.0, 1.0], [488, 1.0, 0.0], [492, 1.0, 0.0], [495, 0.0, 1.0], [503, 0.0, 1.0], [508, 0.0, 1.0], [523, 1.0, 0.0], [527, 0.0, 1.0], [530, 1.0, 0.0], [531, 0.0, 1.0], [532, 0.0, 1.0], [533, 0.0, 1.0], [534, 0.0, 1.0], [541, 1.0, 0.0], [552, 0.0, 1.0], [561, 0.0, 1.0], [569, 0.0, 1.0], [576, 0.0, 1.0], [584, 0.0, 1.0], [600, 1.0, 0.0], [610, 0.0, 1.0], [611, 0.0, 1.0], [614, 1.0, 0.0], [619, 1.0, 0.0], [622, 0.0, 1.0], [627, 0.0, 1.0], [630, 0.0, 1.0], [634, 0.0, 1.0], [638, 0.0, 1.0], [640, 0.0, 1.0], [647, 0.0, 1.0], [650, 0.0, 1.0], [651, 1.0, 0.0], [655, 0.0, 1.0], [657, 0.0, 1.0], [658, 0.0, 1.0], [661, 1.0, 0.0], [664, 1.0, 0.0], [669, 0.0, 1.0], [674, 0.0, 1.0], [680, 0.0, 1.0], [684, 0.0, 1.0], [686, 0.0, 1.0], [687, 0.0, 1.0], [690, 1.0, 0.0], [692, 0.0, 1.0], [697, 0.0, 1.0], [700, 0.0, 1.0], [710, 0.0, 1.0], [713, 0.0, 1.0], [719, 1.0, 0.0], [721, 0.0, 1.0], [722, 0.0, 1.0], [730, 0.0, 1.0], [735, 1.0, 0.0], [746, 0.0, 1.0], [748, 0.0, 1.0], [757, 0.0, 1.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsIr8LlLHooe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}